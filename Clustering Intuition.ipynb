{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CLUSTERING - - PREDICT CATEGORY (UNSUPERVISED)**\n",
    "No dependent variable y at all (i.e., no labels)  \n",
    "Given independent variables x only   \n",
    "Try to learn structure of the data \n",
    "\n",
    "Clustering is a powerful way to explore data and find patterns.      \n",
    "Unsupervised learning is very commonly used with large datasets where it isn't obvious how to start with supervised machine learning.   \n",
    "In general, it's a good idea to try unsupervised learning to explore a dataset before trying to use supervised learning machine learning models.   \n",
    "\n",
    "## Coding \n",
    "### Fit > Transform > Score  \n",
    "\n",
    "---\n",
    "\n",
    "## **K-MEANS**\n",
    "Most basic & most used algorithm for clustering   \n",
    "Basic Premise: To make a prediction, use closest known data points \n",
    "\n",
    "### Process    \n",
    "**1. Assign Data Points**  \n",
    "- Randomly draw k centroids (cluster centers)\n",
    "- Assign each data point to the centroid its closest to \n",
    "\n",
    "**2. Update (Optimize) Centers**\n",
    "- Update the centroids (cluster centers) based on the average position of each of its points \n",
    "    - think rubber bands that like to be as short as possible  \n",
    "    - move cluster to the point where the total rubber band lengths are minimized    \n",
    "    \n",
    "**3. Iterate**\n",
    "- Repeat steps 1 & 2 \n",
    "- keep interating until points stop changing assignments to centroids (alogrithm converges)\n",
    "\n",
    "If you want to predict the cluster for new points, just find the centroid they’re closest to  \n",
    "\n",
    "### Pros:  \n",
    "- Simple to understand, easily adaptable, works well on small or large datasets, fast, efficient and performant      \n",
    "\n",
    "\n",
    "### Cons:  \n",
    "- Need to choose the number of clusters k (Elbow Method). sklearn default is 8.    \n",
    "- Does not attempt to assign any meaning to the clusters you find, it’s up to you to determine that!   \n",
    "- Must scale data to normalize it (important for good results)   \n",
    "- Can't solve donut problem\n",
    "- Sensitive to initial configeration. k-means is a 'hill climbing' algorithm and as a result, it is very dependent on where you put your initial cluster centers.  In other words, highly sensitive to initialization.\n",
    "    - Possible resolution: \n",
    "        - Restart multiple times and use whichever result gives us the best final objective. (What does this tell us? Local Minima)\n",
    "        - Another possible resolution: 'fuzzy' membership in each class (Just a small adjustement to original k-mean algorithm)\n",
    "\n",
    "---\n",
    "## **HIERARCHICAL CLUSTERING**  \n",
    "### Pros:   \n",
    "- The optimal number of clusters can be obtained by the model itself, practical visualization with the dendrogram   \n",
    "\n",
    "### Cons:  \n",
    "- Not appropriate for large datasets  \n",
    "\n",
    "---\n",
    "# OTHER TECHNIQUES\n",
    "- Learning the Structure or Probability Distribution of the Data  \n",
    "- Density Estimation\n",
    "- Latent Variables\n",
    "    - Underlying cause\n",
    "    - Missing or hidden data\n",
    "    - You have a bunch of documents and find distinct clusters\n",
    "\n",
    "## Gaussian Mixture Models (GMM)\n",
    "Give us an approximation of the probability distribution of our data    \n",
    "Gaussian mixture is the sum of weighted Gaussians  \n",
    "Use Gaussian Mixture Models when we notice our data is multi-model (multiple modes/ bumps)\n",
    "- Mode: most common value\n",
    "\n",
    "### Training a GMM \n",
    "Similar to K-Means: \n",
    "- first initialize all parameters to random values\n",
    "- Calculate responsibilities\n",
    "- Calculate model parameters\n",
    "\n",
    "### GMM vs Soft K-Means\n",
    "- K-means looks for clusters of equal weight because it has no PI variable (equivalent to saying pi is uniform or equal to 1 over k)\n",
    "- Can think of soft K-means as a GMM where each cluster has the same weight and is spherical with the same radius\n",
    "\n",
    "## Kernel Density Estimation  \n",
    "- Fitting of a probability with kerns\n",
    "- Easiest Density Estimation: Histogram\n",
    "- Kernel Density Estimation\n",
    "- Just use a Gaussian mixture model\n",
    "- Gaussian = the kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
