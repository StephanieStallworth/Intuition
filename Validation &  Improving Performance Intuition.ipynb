{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION\n",
    "\n",
    "Cross validation = splitting the data into testing and training data:\n",
    "- Gives estimate of performance on an independent dataset\n",
    "- Serves as check on overfitting)\n",
    "\n",
    "We want the model to generalize.  Generalization is being able to predict accurately not only for data we trained on but new data we haven’t seen before.  This data is called 'test data', but if we are using it to choose hyper parameters or a model we might call it ‘validation data’.\n",
    "\n",
    "Usually split data into train/test sets to get an idea of how well a model will generalize.\n",
    "\n",
    "\n",
    "## Train/Test Split\n",
    "Train/Test Split >>>PCA >>>SVM  \n",
    "Training, Transforms, Predicting  \n",
    "\n",
    "**Train/Test Split **   \n",
    "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "**Train** \n",
    "\n",
    "PCA\n",
    "- pca.fit(training_features) # only finding the PC's   \n",
    "- pca.transform(training_features) # transforming data into the PC representation  \n",
    "\n",
    "SVC\n",
    "- svc.train(training_features) \n",
    "\n",
    "\n",
    "**Test**   \n",
    "\n",
    "PCA  \n",
    "- **NO PCA.FIT FOR TEST FEATURES**:Use the SAME PC's found in the training features  \n",
    "- pca.transform(test_features)   # transform test data into PC representation using same PC's found in training data    \n",
    "\n",
    "SVC\n",
    "- svc.predict(test_features)  \n",
    "\n",
    "\n",
    "### Problem with Train/Test Split\n",
    "Problem with splitting into training and testing sets is you want to maximize both training and test set for best learning results but there has to be a tradeoff.\n",
    "\n",
    "Variance Problem: When you test performance of model on one test set, you get one accuracy but if you test it again on another test set you can get a very different accuracy.  So judging accuracy on one test set is not the most relevant way to evaluate model performance.    \n",
    "\n",
    "\n",
    "## K-FOLD CV\n",
    "\n",
    "Cross validation for evaluating algorithm performance  \n",
    "Partition dataset into k bins of equal size  \n",
    "Run k separate learning experiments  \n",
    "- pick one of those k bins as your testing set  \n",
    "- remaining k-1 bins are put together into the training set\n",
    "- test on testing set \n",
    "- Run multiple times and average the 10 different testing set performances   \n",
    "\n",
    "Idea is train and evaluate model using different train/test split combinations (Cross-validation) within the training data, tune hyper-parameters accordingly until you get an acceptable mean accuracy.  Then apply the very best model on test set to see how it performs on ‘general’ data.  \n",
    "\n",
    "### **K-Fold Cross Validation Overview**\n",
    "- Split data into K parts (typical values for K = 5, 8 ,10)  \n",
    "- Loop K times  \n",
    "- In each iteration, take 1 part out (use it for validation), use the rest for training  \n",
    "- Returns K different scores (accuracies)    \n",
    "\n",
    "### **10-Fold Cross Validation Example**\n",
    "- Splitting the training set into 10 folds (most of the time k = 10)  \n",
    "- Train model on 9 folds and test it on the last remaining fold  \n",
    "- With each iteration can use different combinations of the 9 training folds and 1 test fold  \n",
    "- Can train and test the model on 10 combinations of training and test sets  \n",
    "- Take an average of the different accuracies of the 10 evaluations   \n",
    "- and also compute the standard deviation to look at the variance to get a much better idea of model performance  \n",
    "    \n",
    "### **Simpler Variation of K-Folds Cross Validation**  \n",
    "- Instead of evaluating model on one train/test spit, evaluate model on multiple train/test split combinations and take the overall mean accuracy.  \n",
    "- Pass in model and all of your data  \n",
    "- Run model 5 times using different 5 train/test split combinations and returns accuracy for each individual result (Fold) \n",
    " average accuracies together to get an overall error metric  \n",
    "- Evaluate our model against the entire dataset spit up 5 different ways and give us back the individual results  \n",
    "- In practice, you need to try different variations of your model and measure the mean accuracy using K-Fold Cross Validation until you find a sweet spot  \n",
    "\n",
    "### ** K-Fold Cross Validation in sklearn**  \n",
    "from sklearn.model_selection import cross_val_score  \n",
    "\n",
    "#Get the 10 accuracies for each one of the 10 combinations that will be created through k-fold cross validation  \n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train,y= y_train, cv = 10)   # most common choice is 10 folds  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# IMPROVING PERFORMANCE \n",
    "\n",
    "## PARAMETER TUNING\n",
    "Two types of parameters:    \n",
    "\n",
    "### **1. Parameters that the model learned**   \n",
    "\n",
    "### **2. Hyper-parameters ** \n",
    "Parameters that we chose ourselves   \n",
    "Find optimal values of these parameters (Grid Search)\n",
    "Popular method for choosing hyper parameters is k-folds cross validation\n",
    "\n",
    "### GridSearchCV \n",
    "Cross validation for parameter tuning.  \n",
    "Way of systematically working through multiple combos of parameter tunes, cross-validating as it goes, to determine which tune gives the best performance.  \n",
    "\n",
    "**GridSearchCV in sklearn**  \n",
    "parameters = {'kernel':('linear','rbf'),'C':[1,10]}    \n",
    "svr = svm.SVC()    \n",
    "clf = grid_search.GridSearch(svr, parameters)    \n",
    "clf.fit(iris.data, iris.target)    \n",
    "\n",
    "\n",
    "## **XG BOOST**  \n",
    "Most powerful implementation of gradient boosting in terms of model performance and execution speed  \n",
    "1.\tHigh performance  \n",
    "2.\tFast execution speed  \n",
    "3.\tKeep interpretation of problem and model (no feature scaling) \n",
    " \n",
    "\n",
    "## **ENSEMBLE LEARNING**  \n",
    "Using different models to try to solve the same problem and let them vote on the results (Random Forests)  \n",
    "\n",
    "Ensembling techniques take a number of weak learners (classifiers/regressors that are barely better than guessing) combine them (through averaging or max vote) to create a strong learner that can make accurate predictions  \n",
    "\n",
    "### **1.Bagging (Bootstrap aggregating)**  \n",
    "- Take random subsets (bootstrap samples) of the training data and feed them into different versions of the same model and let them all vote on the final result  \n",
    "- Bootstrapping is a type of resampling where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample  \n",
    "- Random forest uses bagging to implement ensemble learning  \n",
    "\n",
    "### **2.Boosting**   \n",
    "- Alternative technique where each subsequent model in the ensemble boost attributes that address data mis-classified by the previous model   \n",
    "- Each model in the ensemble boosts (give more weight to) attributes mis-classified in the previous model so that subsequent models give more focus to them during training  \n",
    "- Keep refining model based on the weaknesses of the previous one  \n",
    "- Uses all the data to train each learner but instances that were misclassified by the previous learners are given more weight so that subsequent learners give more focus to them during training  \n",
    "\n",
    "### **3.A Bucket of Models:**  \n",
    "- trains several different models using training data and picks the one that works best with the test data  \n",
    "- Take entirely different models (for example: Kmeans, decision tree, and regression), run all three models together a on a set of training data and let them all vote on a final classification result   \n",
    "- Pick the model that wins    \n",
    "\n",
    "### **4.Stacking ** \n",
    "- runs multiple models at once on the data and combines the results of all those models together to arrive at a final result  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
