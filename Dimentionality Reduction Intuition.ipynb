{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENTIONALITY REDUCTION INTUITION\n",
    "Remember in Part 3 - Classification, we worked with datasets composed of only two independent variables.  \n",
    "\n",
    "We did for two reasons:  \n",
    "\n",
    "1.Because we needed two dimensions to visualize better how Machine Learning models worked (by plotting the prediction regions and the prediction boundary for each model). \n",
    "\n",
    "\n",
    "2.Because whatever is the original number of our independent variables, we can often end up with two independent variables by applying an appropriate Dimensionality Reduction technique.  \n",
    "\n",
    "There are two types of Dimensionality Reduction techniques:  \n",
    "- Feature Selection  \n",
    "- Feature Extraction  \n",
    "\n",
    "---\n",
    "\n",
    "## **FEATURE SELECTION**    \n",
    "Requires domain knowledge.  \n",
    "We end up with 2 independent variables that are among the original independent variables   \n",
    "See Regression code books  \n",
    "\n",
    "### **Reasons To Ignore A Feature**    \n",
    "It's noisy  \n",
    "It causes overfitting  \n",
    "It is strongly related (highly correlated) with a feature that's already present  \n",
    "Additional features slow down training/testing process  \n",
    "\n",
    "### **Feature =! Information**   \n",
    "Features and Information are two different things.  \n",
    "Features attempt to access information but not exactly the same as the information itself.  \n",
    "Like quantity vs quality\n",
    "Want the bare minimum number of features that gives you as much information as possible.  \n",
    "- Keep too many = over fit  \n",
    "- We want to keep just the most powerful and discriminatory   \n",
    "\n",
    "\n",
    "### Bias-Variance Dilemma and Number of Features  \n",
    "\n",
    "**High Bias:  ** \n",
    "- Pays little attention to data; oversimplified  \n",
    "- Does the same thing over and over again regardless of what the data might be telling it to do  \n",
    "- High error on the training set: low r^2 or large SSE    \n",
    "- When few features are used  \n",
    "\n",
    "**High Variance:**   \n",
    "- Pays too much attention to the data (does not generalize well); overfits  \n",
    "- Just memorizing the training examples and as soon as it gets a new data point that isn't like the training examples, doesn't know what to do   \n",
    "- Much higher error on the test set than on training set  \n",
    "- Using many feautres, carefully optimized performance on training data  \n",
    "- Classic way to overft an algorithm is by using alot of features and not alot of training data  \n",
    "\n",
    "**Sweet Spot: want to fit an algorithm with ** \n",
    "- Few features \n",
    "- Large r^2 or low SSE \n",
    " \n",
    "### **Univariate Feature Selection**  \n",
    "There are several go-to methods of **automatically** selecting your features in sklearn. Many of them fall under the umbrella of **univariate feature selection, which treats each feature independently** and asks how much power it gives you in classifying or regressing.\n",
    "\n",
    "There are two big univariate feature selection tools in sklearn: SelectPercentile and SelectKBest. The difference is pretty apparent by the names:   \n",
    "- **SelectPercentile:** selects the X% of features that are most powerful (where X is a parameter)   \n",
    "    from sklearn.feature_selection import SelectPercentile, f_classif  \n",
    "    selector = SelectPercentile(f_classif, percentile = 10)    \n",
    "    \n",
    "    \n",
    "- **SelectKBest:** selects the K features that are most powerful (where K is a parameter).\n",
    "\n",
    "\n",
    "A clear candidate for feature reduction is text learning, since the data has such high dimension.\n",
    "\n",
    "### **Other Feature Selection Techniques:**\n",
    "- Backward Elimination,\n",
    "- Forward Selection, \n",
    "- Bidirectional Elimination, \n",
    "- Score Comparison and more.   \n",
    " \n",
    "**Greedy Method**\n",
    "- Build a classifier for each individual feature, pick the best one via cross-validation  \n",
    "- Build another set of classifier, all of which contain the first (best) feature, and one other feature. \n",
    "- Pick the best via cross-validation. Now you have two features  \n",
    "- Repeat.  \n",
    "\n",
    "### Feature Selection (aka Regularization) in Regression \n",
    "Power of regualization is that it can automatically do this selection for you.   \n",
    " \n",
    "**Ordinary Multivariate Regression v Lasso Regression **    \n",
    "\n",
    "**Regular Linear Regression:**  \n",
    "Just want to minimize SSE   \n",
    "Uses all the features made available to it and it'll assign each one a coefficient of regression \n",
    "\n",
    "**Lasso regression:**  \n",
    "Regularized regression  \n",
    "Method for automatically penalizing extra features(for features that don't help the regression results enough, can set coefficient to a feature to zero)  \n",
    "\n",
    "In addition to minimizing SSE, also what to minimize the number of features that I'm using.  So add in a penalty parameter for additional features.    \n",
    "- The gain in terms of precision/goodness of fit has to be bigger than the loss that I take as a result of having that additional feature in my regression.    \n",
    "- Automatically takes in account the penalty parameter and in so doing, it helps you figure out which feature has the most important effect on the regression and elimiate (or set to zero) the coefficients to the features that basically don't help.  \n",
    "\n",
    "Will try adding features in one at a time and if the new feature does't improve the fit enough to outweigh the penalty term of including that feature then it won't be added (coefficient is set to zero).\n",
    "\n",
    "**Lasso in sklearn**  \n",
    "#Fit  \n",
    "import sklearn.linear_model.Lasso   \n",
    "features, labels = GetMyData()    \n",
    "regression = Lasso()  \n",
    "regression.fit(features, labels)   # supervised learning so need to fit with features and labels \n",
    "\n",
    "#Predict    \n",
    "regression.predict([2,4]) #pass features you want to make predictions for \n",
    "\n",
    "#Score    \n",
    "print regression.coef_   # to see which features have large coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FEATURE EXTRACTION**  \n",
    "\n",
    "## **1.\tPRINCIPAL COMPONENT ANALYSIS (PCA)**   \n",
    "\n",
    "### Overview  \n",
    "PCA is not a full machine learning algorithm, but instead an unsupervised learning algorithm. It is a transformation of the data and attempts to identify what features explain the most variance in the data.\n",
    "\n",
    "From the m independent variables of your dataset, PCA extracts p<= m **NEW** independent variables that explain the most variance of the dataset, regardless of the dependent variable.    \n",
    "\n",
    "End up with 2 independent variables that are **NEW** (as opposed to feature selection where we end up with 2 independent variables that are among the original independent variables).     \n",
    "\n",
    "The fact that DV (dependent variable) is not considered makes PCA an unsupervised model.  \n",
    "\n",
    "Apply PCA after data pre-processing and scaling, but **before** you fit the model.   \n",
    "\n",
    "### **PCA is a Powerful Algorithm**\n",
    "- For dimensionality reduction. Can PCA to bring down the dimensionality of your features to turn a bunch of features into just a few.  \n",
    "- Also powerful as a stand alone method in its own right for unsupervised learning  \n",
    "\n",
    "\n",
    "### Review/Definition of PCA  \n",
    "1. Systematized way to transform input features into their principal components   \n",
    "\n",
    "2. Use those Principal Components as new features (the principal coponents are available to you to use instead of original input features)\n",
    "\n",
    "3. Principal Components are directions in data that maximize variance (minimize information loss) when you project/compress down onto them.   \n",
    "\n",
    "4. More variance of data along a PC, higher that PC is ranked \n",
    "    - Most variance/most information = First PC\n",
    "    - Second  mostvariance (without overlapping with first PC) = Second PC    \n",
    "\n",
    "5. Max Number of PC's = number of input features you had in your data set\n",
    "    - Usually only use the first handful of Principal Components  \n",
    "    - Could go all the way out and use the max number, but in that case, you're not really gaining anything. Just representing your features in a different way.   \n",
    "\n",
    "### When to use PCA  \n",
    "**Latent features driving the patterns in data (big shots @ Enron)**  \n",
    "- if you want access to latent features you think might be showing up in the data\n",
    "- or trying to figure out if there is a latent feature (size of first PC)  \n",
    "\n",
    "**Dimensionality Reduction **  \n",
    "- **Help you visualize high-dimensional data**  \n",
    "When you draw a scatter plot, only have two dimensions that are available to you but many times you'll have more than two features.  So there's a struggle of how to represent 3 or 4 numbers about a data poitn if you only have two dimension in which to draw it. What you can do, is you can project it down to the first 2 PC's and just plot that.  So things like K-means can be alot easier to visualize.  Still capturing most of the info in the data but now you can draw it with those two dimensions.  \n",
    "    \n",
    "    \n",
    "- **Reduce noise(almost all data will have noise).**  \n",
    "The first and second PC's are capturing the actual patterns in the data and the smaller PC's are just representing noisy variations about those patterns.  By throwing away the less important PC's ,gettgn rid of that noise.  \n",
    "    - Make other algorithms (regression, classification) work better because few inputs (eigenfaces)  \n",
    "    - Use PCA as pre-processing before you use another alogirthm   \n",
    "\n",
    "### ** Pros: **\n",
    "All outputs are uncorrelated (no redundancy)  \n",
    "Outputs are sorted by information contained (measure by variance)  \n",
    "We choose enough features such that we retain 95% or 99% of the original variance (this is feature selection)    \n",
    "Automatic: doesn’t require domain knowledge      \n",
    "Get a good idea of what it is so you can use it everytime you want to visualize data  \n",
    "\n",
    "### **Cons: **\n",
    "It’s only a linear transformation  \n",
    "\n",
    "### **PCA for Data Transformation**  \n",
    "If you're given data of any shape, PCA  finds the new coordinate system that's obtained from the old one (by translation and rotation only) and then...\n",
    "- Moves the center of the coordinate system with the center of the data  \n",
    "- Moves the x axis into the pricinal axis of variation (where you see the most variation relative to all the data points) \n",
    "- Moves the further axis so it is orthogonal (makes a right angle) in the less important direction of variation  \n",
    "\n",
    "PCA finds for you these axis and also tells you how important these axes are (importance value/spread value)  \n",
    "\n",
    "### ** Measurable vs Latent Features**  \n",
    " \n",
    "Business Problem: Given the features of a home, what is its price?  \n",
    "\n",
    "**Measurable Variables**  \n",
    "sq footage, numbers of rooms, school ranking, neighborhood safety  \n",
    "\n",
    "**Latent Variable**   \n",
    "Variables that you can't measure directly, but are driving the phenomenon you're measuring behind the scenes.  \n",
    "Even though you're measuring all these things (sq footage, # of rooms, school ranking, neighborhood safety), you're only really probing two things: size of home and neighborhood.\n",
    "\n",
    "What is the best way to condense our 4 features into 2 so that we really get to the heart of the information (probing the size and neighborhood)? \n",
    "\n",
    "If you have many features, but hypothesize a smaller number of features actually driving the patterns.  Try making a composite feature (called principle component) that more directly probes the underlying phenomenon.  \n",
    "\n",
    "\n",
    "### How PCA works  \n",
    "Does a projection of a two dimensional feature space into one dimension.      \n",
    "Data points project shadows onto a line (the Principal Component) and then that line is turned sideways so it's 1 dimensional.  \n",
    "\n",
    "Projected line is the Principal Component  \n",
    "Project down datapoints onto that Principal Component.  \n",
    "\n",
    "**Housing Prices Example**  \n",
    "Combining number of rooms and sq footage into a size feature.   \n",
    "Combining safety problems and school ranking into one feature that roughly gauges the quality of the neighborhood.    \n",
    "\n",
    "\n",
    "### **How to Determine the Principal Component**  \n",
    "Variance in ML is the willingness/flexiblity of an algorithm to learn  \n",
    "Variance is also a technical term in statistics: roughly the spread of the data distribution (similar to standard deviation). \n",
    "\n",
    "What determines the Principal Component of a dataset is the direction that has the largest variance in the data  \n",
    "Can look simliar to a regression, but PCA is trying to find the direction of maximum variance (not make a prediction!)  \n",
    "- Use this direction because it retains most amount of 'information' in the original data   \n",
    "- We project along the direction of the largest variance because this retains the max amount of info in the original data.  \n",
    "    - Amount of information lost is the distance from data point in the 2d space to new projected spots on the one dimensional line (Principal Component). \n",
    "    - Projection is made onto direction of maximal variance that minimizes distance from old (higher dimensional) data and its new transformed value - thereby minimizing information loss.  \n",
    "    \n",
    "### Selecting a Number of Principal Components  \n",
    "Train on different number of PC's and see how accuracy responds - cut off when it becomes apparent that adding more PC's doesn't buy you much more discrimination.  \n",
    "\n",
    "### PCA >  Feature Selection\n",
    "**DO NOT** want to perform feature selection before you go into PCA.  PCA is going to find a way to combine info from potentally many different features together. So if you throw out information before you do PCA, you're throwing out information PCA might be able to rescue in a sense.  You can do feature selection on the PC's **AFTER** you make them.   \n",
    "\n",
    "PCA can be computational expensive so an exception might be if you have a large input feature space and you know alot of them are completely irrelevant -- go ahead and toss them out but **proceed with caution**.  \n",
    "\n",
    "### PCA as a General Algorithm for Feature Transformation    \n",
    "PCA is a powerful unsupervised learning technique that you can use to fundamentaly understand the latent features in your data. \n",
    "If you knew nothing about housing prices, PCA could give you insight that there are two things that seem to drive house prices in general.    \n",
    "\n",
    "Put all the features into the PCA together and it can automatically \n",
    "- Combine them into new features and \n",
    "- Rank the relative power of those features.  \n",
    "    - First PC: most effect; harder to make interpretations because it could be a mixture that contains bits and pieces from potentially all of the features    \n",
    "    - Second PC... and so on \n",
    "\n",
    "\n",
    "### PCA in sklearn  \n",
    "from sklearn.decomposition import PCA          \n",
    "pca = PCA(n_components = 2)        \n",
    "pca.fit(data)        \n",
    "return pca     \n",
    "\n",
    "pca = doPCA()   \n",
    "print pca.explained_variance_ratio_    \n",
    "first_pc = pca.components_[0]      \n",
    "second_pc = pca.components_[1]    \n",
    "\n",
    "transformed_data = pca.transform(data)    \n",
    "for ii, jj in zip(transformed_data, data):    \n",
    "    plt.scatter( first_pc[0]*ii[0], first_pc[1]*ii[0], color = 'r')  \n",
    "    plt.scatter( second_pc[0]*ii[1], second_pc[1]*ii[1], color = 'c')  \n",
    "    plt.scatter( jj[0], jj[1], color = 'b')    \n",
    "    \n",
    "plt.xlabel('bonus')    \n",
    "plt.ylabel('long-term incentive')    \n",
    "plt.show()  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **2.\tLINEAR DISCRIMINANT ANALYSIS (LDA)**  \n",
    "a.\tFrom the n independent variables of the dataset, LDA extracts p <= n new independent variables that separate the most of the classes of the dependent variable  \n",
    "b.\tConsiders the dependent variable, makes LDA a supervised model    \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **3.\tKERNEL PCA**  \n",
    "a.\tNon-linear feature extraction model  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **4.\tQUADRATIC DISCRIMINANT ANALYSIS (QDA)**  \n",
    "\n",
    "Dimensionality reduction attempts to distill higher-dimensional data down to a smaller number of dimensions, while preserving as much of the variance in the data as possible.    \n",
    "•\tlets you distill multi-dimensional data down to fewer dimensions, selecting new dimensions that preserve variance in the data as best it can.    \n",
    "•\tDimensions: features    \n",
    "•\tExplained variance ratio: how much of the variance in the original data was preserved as I reduced it down to two dimensions\n",
    "•\tPCA has chosen the remaining two dimensions well enough that we've captured 92% of the variance in our data in a single dimension alone! The second dimension just gives us an additional 5%; altogether we've only really lost less than 3% of the variance in our data by projecting it down to two dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
