{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CLASSIFICATION – PREDICT CATEGORY (SUPERVISED)  **  \n",
    "\n",
    "Dependent variable y (outcome we're trying to predict) is a category  \n",
    "Given input features (Independent variables x) and try to produce discrete labels (dependent variable y)    \n",
    "\n",
    "## **Properties**\n",
    "- Output Type: Discrete (class labels)\n",
    "- What you're trying to find: Decision Boundary\n",
    "- Evaluation: Accuracy  \n",
    "\n",
    "## Examples\n",
    "ML Algorithm takes data and transforms it into a decision surface that for all future cases can enable us to make a determination what class it is in.  \n",
    "Try to learn to predict target (Y) given input (X)\n",
    "\n",
    "- Whether or not it will rain tomorrow  \n",
    "- Whether or not Google’s stock price will rise or fall tomorrow  \n",
    "- Whether Katie will like or dislike a song\n",
    "    - We don't input raw music instead we extract 'features' (insensity, tempo, genre, gender) from the music\n",
    "    - then Katie's brain processes it into one of two categories: Like and Don't like.  \n",
    "\n",
    "## Descriminative vs Generative Classifiers\n",
    "### **Discriminative**\n",
    "- We start with X, we get Y\n",
    "- Classifiers like logistic regression model this directly (discriminative)  \n",
    "    \n",
    "### **Generative**\n",
    "- We start with Y(the class) and model X  \n",
    "- Think of each class as a ‘data-making machine’  (it ‘generates’ the data)   \n",
    "- Naïve Bayes    \n",
    "\n",
    "## Coding \n",
    "### **Split > Fit > Predict > Score It**   \n",
    "Fit aka 'train'  \n",
    "Algorithms that can learn from observational data and make predictions based on it  \n",
    "\n",
    "### **Using 2 Main Functions **   \n",
    "train(x,y)  \n",
    "predict(x)\n",
    "\n",
    "\n",
    "### **Data Types and Shapes**    \n",
    "Sci-kit learn requires everything to be numerical  \n",
    "\n",
    "x is a **MATRIX** of N x D (two brackets)\n",
    "X = dataset[['feature1','feature2','feature3']]   \n",
    "- n = # of observations\n",
    "- d = number of features (dimensions or columns)\n",
    "\n",
    "y is a **VECTOR** of shape N x 1 (1 bracket)  \n",
    "y = dataset['target']\n",
    "- for a classification y will contain discrete integers from 0...k-1 where k is the number of classes\n",
    "\n",
    "--- \n",
    "\n",
    "## **1.Logistic Regression  - - LINEAR**    \n",
    "a.\tPros: Probabilistic approach, gives information about statistical significance of features  \n",
    "b.\tCons: The Logistic Regression Assumptions  \n",
    "c.\tUse Logistic Regression (or Naive Bayes if problem is non-linear) when you want to rank your predictions by their   probability.   \n",
    "-For example if you want to rank your customers from the highest probability that they buy a certain product, to the   lowest probability.   \n",
    "-Eventually that allows you to target your marketing campaigns. And of course for this type of business   problem, you should use Logistic Regression if your problem is linear  \n",
    "\n",
    "---\n",
    "\n",
    "## **2.Support Vector Machine (SVM) -- LINEAR**    \n",
    "When you want to predict to which segment your customers belong to. \n",
    "Use SVC to classify data using SVM      \n",
    "Can be any kind of segments, for example  some market segments you identified earlier with clustering.  \n",
    "Can use different ‘kernels’ (linear, rbf, polynomial) . Some will work better than others for a given data set    \n",
    "Put first and foremost the classification of the labels, then maximizes margin\n",
    "- Margin = distance between the line and the nearest point of both of the two classes    \n",
    "- want to maximize this because is where it is most robust  \n",
    "\n",
    "Pros:   \n",
    "- Performant, not biased by outliers, not sensitive to overfitting  \n",
    "- Works well for classifying higher dimensional data (lots of features)\n",
    "\n",
    "Cons:   \n",
    "- Not appropriate for non linear problems, not the best choice for large # of features  \n",
    "- Need to scale features  \n",
    " \n",
    "#Linear SVM  \n",
    "\n",
    "#Split  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .20, random_state= 0)  \n",
    "\n",
    "#Fit  \n",
    "from sklearn.svm import SVC  \n",
    "clf = SVC(kernel = 'linear')  \n",
    "clf.fit(features_train, labels_train)  \n",
    " \n",
    "#Predict  \n",
    "pred = clf.predict(features_test)  \n",
    "\n",
    "#Score  \n",
    "from sklearn.metrics import accuracy_score  \n",
    "acc = accuracy_score(pred, labels_test)  \n",
    "\n",
    "---\n",
    "\n",
    "## **3.K-Nearest Neighbors (K-NN) – NON LINEAR**    \n",
    "Used to classify new data points based on ‘distance’ to known data on a scatter plot  \n",
    "- Find K nearest neighbors, based on your distance metric  \n",
    "- Let them all vote on the classification  \n",
    "\n",
    "Pros: Simple to understand, fast and efficient  \n",
    "Cons: Need to choose the number of neighbours k  \n",
    "\n",
    "\n",
    "--- \n",
    "## **4.Kernel SVM – NON LINEAR**     \n",
    "\n",
    "### **Kernel Trick**  \n",
    "There are functions that take a low dimensional input space (feature space) and map it to a very high dimensional space.   \n",
    "So what use to not be linearably separable turns it into a seperable problem.     \n",
    "\n",
    "These functions are called Kernels \n",
    "\n",
    "Find the best linear separator between the different classes, apply the kernel trick in a high dimensional space and what you effectively get is a very powerful system to set data sets apart where the division line might be non-linear.  \n",
    "                     \n",
    "x, y not sepearable  > Kernels > x1 x2 x3 x4 x5  separable\n",
    "\n",
    "\n",
    "Steps when you apply the kernel trick:  \n",
    "- change your input space from x, y into a much larger input space >   \n",
    "- separate the data points using support vector machines >  \n",
    "- then take the solution and go back to the original space >   \n",
    "- you now have a non-linear separation.   \n",
    "\n",
    "\n",
    "### **Pros: **  \n",
    "- High performance on nonlinear problems, not biased by outliers, not sensitive to overfitting \n",
    "- work well in complicated domains where there is a clear margin of separation \n",
    "\n",
    "### **Cons:**   \n",
    "- Very big data set with lots and lots of features, SVM right out of the box might be very slow and prone to overfitting to the noise in your Data. \n",
    "- Doesn't work well in very large data sets \n",
    "- Not the best choice for large number of features, more complex  \n",
    "- Doesn't work well with lots of noise \n",
    "    - where the classes are very overlapping, have to count independent evidence.   \n",
    "    - That's when a naive bayes classifier would be better   \n",
    "- Need to scale features \n",
    "\n",
    "### Parameters in Machine Learning\n",
    "Arguments passed when you create your classifier (before fitting)    \n",
    "For SVM: Tuning kernel, C, and gamma all help with overfitting  \n",
    "\n",
    "**1. Kernel**   \n",
    "- Kernels options: linear, poly, rbf, sigmoid, precomputed, or a callable\n",
    "\n",
    "**2. C:** controls tradeoff between a smooth decision boundary and one that classifies all the training points correctly  \n",
    "- something very straight but coems at the cost of afew points being missed classified\n",
    "- something wiggly but where you get potentiall all of the training points correct\n",
    "    -something complicated like, chances are it this won't generalize that well to test set \n",
    "- Large value of C means you're going to get more traiing pionts correct (more intricate decision boundaries where it can wiggle around individual data points to get everything correct)  \n",
    "\n",
    "**3. Gamma:** defines how far the influence of a single training example reaches\n",
    "- low values - far reach (even the far away get taken into consideration when deciding where to draw decision boundary)\n",
    "    -makes decision boundary more linear, smoother, less jagged\n",
    "- high values - close reach (ignores points that are farther away)  \n",
    "    - can end up with a wiggly decision boundary\n",
    "\n",
    "---\n",
    "\n",
    "## **5.Naive Bayes – NON LINEAR**     \n",
    "Use Naive Bayes (if problem is non-linear) when you want to rank your predictions by their probability. For example if you   want to rank your customers from the highest probability that they buy a certain product, to the lowest probability. Eventually  that allows you to target your marketing campaigns. And of course for this type of business problem, Naive Bayes if your   problem is non linear (Logistic Regression if your problem is linear).    \n",
    "\n",
    "One particular feature of Naive Bayes is that it’s a good algorithm for working with text classification. When dealing with text, it’s very common to treat each unique word as a feature, and since the typical person’s vocabulary is many thousands of words, this makes for a large number of features. The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts.\n",
    "\n",
    "### Pros:   \n",
    "- Efficient, not biased by outliers, works on nonlinear problems, probabilistic approach  \n",
    "- Grounded in probability, which can be powerful  \n",
    "\n",
    "### Cons:  \n",
    "- Based on the assumption that features have same statistical relevance  \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB    \n",
    "clf = GaussianNB()  \n",
    "clf.fit(features_train, labels_train)  \n",
    "pred = clf.predict(features_test)  \n",
    "\n",
    "### Bayes Rule\n",
    "Prior Probability * Test Evidence = Posterior Probability  \n",
    "\n",
    "### Non-Naive Bayes    \n",
    "- Usually we just call it ‘Bayes Classifier’  \n",
    "- More generally we can have a ‘Bayes Model’ \n",
    "\n",
    "\n",
    "### Naive Bayes - Text Learning Example  \n",
    "With naïve Bayes we assume all of the features are independent  \n",
    "\n",
    "Chris\n",
    "- Love .10\n",
    "- Deal .80\n",
    "- Life .10  \n",
    "\n",
    "Sara\n",
    "- Love .50  \n",
    "- Deal .20  \n",
    "- Life .30    \n",
    "\n",
    "**Text: \"Life Deal\"**     \n",
    "\n",
    "**Prior Probability**  \n",
    "P(Chris) = 0.50    \n",
    "P(Sara) = 0.50    \n",
    "\n",
    "**Evidence * Prior Probability**  \n",
    "Chris: .10 x .80 x .50 = 0.04<<<<Chris wrote it    \n",
    "Sara: .30 x .20 x .50 = 0.03 \n",
    "\n",
    "**Posterior Probability**     \n",
    "P(Chris | 'Life Deal') = 0.04 / (0.04 + 0.03) = 0.57    \n",
    "P(Sara | 'Life Deal') =0.03/(0.04 + 0.03) =  0.43  \n",
    "\n",
    "Text: \"Love Deal\"  \n",
    "\n",
    "**Prior Probability**            \n",
    "P(Chris) = 0.50    \n",
    "P(Sara) = 0.50    \n",
    "\n",
    "**Evidence * Prior Probability**  \n",
    "P(Chris| 'Love Deal') = 0.10 x 0.80 x 0.50 = 0.04    \n",
    "P(Sara |'Love Deal') = 0.50 x 0.20 x 0.50 = 0.05   \n",
    "\n",
    "**Posterior Probability**  \n",
    "P(Chris | 'Love Deal') = 0.04 / (0.04 + 0.05) = 0.444      \n",
    "P(Sara | 'Love Deal') =0.05/(0.04 + 0.05) =  0.555 <<<Sara wrote it \n",
    "\n",
    "### Why is Naive Bayes Naive?   \n",
    "- Because it ignores Word Order. Just looks at frequency \n",
    "-\n",
    "### Naive Bayes Strengths and Weaknesses   \n",
    "- Easy to implement  \n",
    "- Can break with phrases    \n",
    " \n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **6.Decision Tree Classification – NON LINEAR**   \n",
    "Decision trees uses a trick that lets you do non-linear decision making with simple linear decision surfaces.   \n",
    "Allows you to ask multiple linear questions, one after the other.    \n",
    "Basically a bunch of nested if statements   \n",
    "\n",
    "What makes it ML is how we choose the conditions  \n",
    "- Based on information theory  \n",
    "- We only look at one attribute at a time (Usually call these ‘input features’ but called ‘attributes’ when talking about decision trees)    \n",
    "  \n",
    "### Entropy  \n",
    "A measure of a dataset’s disorder – how same or different it is   \n",
    "Measure of impurity in a bunch of examples    \n",
    "\n",
    "If we classify a dataset into N different classes (ex: a data set of animal attributes and their species)  \n",
    "- Entropy = 0: All examples are the same class (everyone is an iguana)   \n",
    "- Entropy = 1: They’re all different classes  \n",
    "\n",
    "Controls how a DT decides where to split the data (want to minimize impurity in the splitting)\n",
    "\n",
    "\n",
    "### Information Gain   \n",
    "Decision tree looks at all the training examples, all of the different features that are available to it and it uses information gain criterion in deciding which variable to split on, and how to make the splits.  \n",
    "- Decision tree algorithm: maximize information gain  \n",
    "- This is how it will choose which feature to make a split on  \n",
    "\n",
    "Information Gain = entropy (p) - [weighted average]entropy(children)  \n",
    "\n",
    "import scipy.stats\n",
    "print scipy.stats.entropy([2,1],base=2)    \n",
    "      \n",
    "### Bias-Variance Dilemma  \n",
    "**High Bias ML algorithm:  **\n",
    "- one that practially ignores the data\n",
    "- almost no capacity to learn anything \n",
    "- Ex: bias car, no matter which way I train it - doesn't do anything differently    \n",
    "\n",
    "**High Variance ML algorithm:**\n",
    "- extremely perceptive to data   \n",
    "- can only replicate stuff its seen before  \n",
    "- reacts very poorly to situations it hasn't seen before because it doesn't have the right bias to generalize to new stuff.   \n",
    "\n",
    "\n",
    "**Want something in the middle**\n",
    "- has some authority to generalize but is still very open to listen to the data  \n",
    "\n",
    "\n",
    "### Pros: \n",
    "- Interpretability, no need for feature scaling, works on both linear / nonlinear problems  \n",
    "- Scaling optional  \n",
    "- Use when you want to have clear interpretation of your model results.\n",
    "\n",
    "### Cons:  \n",
    "- Poor results on too small datasets, overfitting can easily occur \n",
    "- Susceptible to overfitting; especially if the data has lots and lots of features     \n",
    "- Decision trees have a major flaw—they overfit to the training data.   \n",
    "    - Because we build up a very \"deep\" decision tree in terms of splits, we end up with a lot of rules that are specific to the quirks of the training data, and not generalizable to new data sets.  \n",
    "    - one of the easiest ways to get an overfit decision tree is to use a small training set and lots of features.\n",
    "\n",
    "### Speeding Up Performance  \n",
    "- A general rule is that the parameters can tune the complexity of the algorithm, with more complex algorithms generally running more slowly.\n",
    "\n",
    "- Another way to control the complexity of an algorithm is via the number of features that you use in training/testing. The more features the algorithm has available, the more potential there is for a complex fit.\n",
    "\n",
    "### Decision Tree Parameters    \n",
    "\n",
    "**min_samples_split: **governs whether there is enough samples available to me continue to split further (default is 2; higher # more simple the boundary)   \n",
    "\n",
    "### Tuning Criterion Parameter  \n",
    "default= 'gini' index\n",
    "Another similar metric of impurity  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **7.Random Forest Classification – NON LINEAR**  \n",
    "'ensemble method'  \n",
    "meta classifier built from (usually) decision trees  \n",
    "\n",
    "Build hundreds of trees with slightly randomized input data, and slightly randomized split points. Each tree in a random forest gets a random subset of the overall training data. The algorithm performs each split point in each tree on a random subset of the potential columns to split on. By averaging the predictions of all of the trees, we get a stronger overall prediction and minimize overfitting.\n",
    "\n",
    "Construct several alternate decision trees and let them “vote” on the final classification  \n",
    "- Randomly re-sample the input data for each tree   \n",
    "- Called “bootstrap aggregating” or “bagging”  \n",
    "- Randomize a subset of the attributes each step is allowed to choose from \n",
    "\n",
    "### **When To Use Random Forests**\n",
    "While the random forest algorithm is incredibly powerful, it isn't applicable to all tasks. \n",
    "\n",
    "### **The main strengths of a random forest are:**    \n",
    "- Pros: Powerful and accurate, good performance on many problems, including non linear\n",
    "\n",
    "\n",
    "- Very accurate predictions - Random forests achieve near state-of-the-art performance on many machine learning tasks. Along with neural networks and gradient-boosted trees, they're typically one of the top-performing algorithms.  \n",
    "\n",
    "\n",
    "- Resistance to overfitting - Due to their construction, random forests are fairly resistant to overfitting. We still need to set and tweak parameters like max_depth though.\n",
    "\n",
    "### **The main weaknesses of using a random forest are:**  \n",
    "- Cons: No interpretability, overfitting can easily occur, need to choose the number of trees    \n",
    "\n",
    "\n",
    "- They're difficult to interpret - Because we've averaging the results of many trees, it can be hard to figure out why a random forest is making predictions the way it is.\n",
    "\n",
    "\n",
    "- They take longer to create - Making two trees takes twice as long as making one, making three takes three times as long, and so on. Fortunately, we can exploit multicore processors to parallelize tree construction. Scikit allows us to do this through the n_jobs parameter on RandomForestClassifier..\n",
    "\n",
    "### **Random Forest v Decision Tree Bottom Line**    \n",
    "**Random Forest**  \n",
    "Given these trade-offs, it makes sense to use random forests in situations where accuracy is of the utmost importance; being able to interpret or explain the decisions the model is making isn't key. \n",
    "Use when you are just looking for high performance with less need for interpretation.   \n",
    "\n",
    "\n",
    "**Decision Tree**  \n",
    "In cases where time is of the essence or interpretability is important, a single decision tree may be a better choice.\n",
    "\n",
    "---\n",
    "\n",
    "# MODEL EVALUATION  \n",
    "\n",
    "## Confusion Matrices  \n",
    "2 x 2 matrix that compares actual class v prediced class \n",
    "\n",
    "## Accuracy  \n",
    "Accuracy = All data points labeled correctly / All data points\n",
    "\n",
    "**Pros**  \n",
    "Simple metric  \n",
    "\n",
    "**Cons**    \n",
    "Not ideal for skewed classes    \n",
    "\n",
    "## Recall (aka Sensitivity)\n",
    "Out of all the TP items, how many were correctly classified.  How many TP items were 'recalled' from the data set\n",
    "TP / (TP + FN)  \n",
    " \n",
    "\n",
    "##   Precision     \n",
    "Out of all the items labeled as p, how many truely belong in the p class.\n",
    "TP / (TP + FP)  \n",
    "\n",
    "\n",
    "There is usually a trade-off between precision in recall.  \n",
    "\n",
    "## F-1 Score    \n",
    "\n",
    "Considers both the precision and recall to compute the score\n",
    "\n",
    "F1 = (2)(Precision x Recall)/(Precision + Recall)  \n",
    "\n",
    "This is the best of both words:  \n",
    "- If my identifier finds a Person of Interest then the person is almost certainly a Person of Interest and  \n",
    "- if the identifier does not flag someone, then they are almost certainly not a Person of Interest.  \n",
    "\n",
    "In a multiclass classification problem like this one (more than 2 labels to apply), accuracy is a less-intuitive metric than in the 2-class case. Instead, a popular metric is the F1 score.\n",
    "\n",
    "We’ll learn about the F1 score properly in the lesson on evaluation metrics, but you’ll figure out for yourself whether a good classifier is characterized by a high or low F1 score. You’ll do this by varying the number of principal components and watching how the F1 score changes in response.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **IMPROVING PERFORMANCE** \n",
    "\n",
    "### **Tuning Parameters To Improve Accuracy**\n",
    "\n",
    "- The first (and easiest) thing we can do to improve the accuracy of the random forest is to increase the number of trees we're using. Training more trees will take more time, but because we're averaging many predictions we made on different subsets of the data, having more trees will greatly increase accuracy (up to a point).\n",
    "\n",
    "\n",
    "- We can also tweak the min_samples_split and min_samples_leaf variables to reduce overfitting. Because of the way a decision tree works, very deep splits in a tree can make it fit to quirks in the data set, rather than true signal. For this reason, increasing min_samples_split and min_samples_leaf can reduce overfitting. This will actually improve our score because we're making predictions on unseen data. A model that's less overfit and can generalize better will actually perform better on unseen data, but worse on seen data. \n",
    "\n",
    "- Popular method for choosing hyper parameters is k-folds cross validation\n",
    "\n",
    "- Can simply use the mean   \n",
    "\n",
    "- Also use statistical testing to check if one 1 parameter setting is ‘statistically significantly’ better than the other\n",
    "\n",
    "\n",
    "### **Identifying the Best Features to Use**  \n",
    "- Feature engineering is the most important part of any machine learning task, and there are a lot more features we could calculate. However, we also need a way to figure out which features are the best.\n",
    "\n",
    "\n",
    "- One way to accomplish this is to use univariate feature selection. This approach essentially involves reviewing a data set column by column to identify the ones that correlate most closely with what we're trying to predict (Survived).\n",
    "\n",
    "\n",
    "- As usual, sklearn has a function that will help us with feature selection. The SelectKBest function selects the best features from the data. We can specify how many features we want this function to select.\n",
    "\n",
    "### ** Gradient Boosting**  \n",
    "- Another technique that builds on decision trees is gradient boosting. Boosting involves training decision trees one after another, and feeding the errors from one tree into the next tree. \n",
    "\n",
    "\n",
    "- This method allows each tree to build on all the ones that came before it. Using this method can lead to overfitting if we build too many trees, though. As we get above 100 trees, it's very easy to overfit and train to quirks in the data set. Because our data set is extremely small, we'll limit the tree count to 25.\n",
    "\n",
    " \n",
    "- Another way to reduce overfitting is to limit the depth to which we can build each tree in the gradient boosting process. We'll limit the tree depth to 3 here to avoid overfitting.\n",
    "\n",
    "### ** Making Predictions with Multiple Classifiers (Ensembling)**  \n",
    "- One thing we can do to improve the accuracy of our predictions is ensemble different classifiers. Ensembling means generating predictions based on information from a set of classifiers, instead of just one. In practice, this means that we average their predictions.\n",
    "\n",
    "\n",
    "- Generally speaking, the more diverse the models we ensemble, the higher our accuracy will be. Diversity means that the models generate their results from different columns, or use very different methods to generate predictions. Ensembling a random forest classifier with a decision tree probably won't work extremely well, because they're very similar. On the other hand, ensembling a linear regression with a random forest can yield very good results.\n",
    "\n",
    "\n",
    "- One caveat with ensembling is that the classifiers we use have to be about the same in terms of accuracy. Ensembling one classifier that's much less accurate than the other will probably make the final result worse.\n",
    "\n",
    "\n",
    "- In this case, we'll ensemble logistic regression we trained on the most linear predictors (the ones that have a linear order, as well as some correlation to Survived) with a gradient-boosted tree we trained on all of the predictors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
