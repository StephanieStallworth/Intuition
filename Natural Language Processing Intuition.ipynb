{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING\n",
    "Applying Machine Learning models to text and language \n",
    "\n",
    " \n",
    "## TRAINING NLP MODEL  \n",
    "\n",
    "## Step 1: Stemming   \n",
    "Way to bundle similar words and representing them as a single word\n",
    "Use algorithm called a 'stemmer'  that applies a function to words that will strip them down so they have the same root.   \n",
    "Stemming consolidates vocabulary for better processing    \n",
    "\n",
    "### **Stemming with NLTK**  \n",
    "from nltk.stem.snowball import SnowballStemmer  \n",
    "stemmer = SnowballStemmer('english')    \n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Remove Stopwords  \n",
    "Stopword: low information, highly frequent word\n",
    "Common to remove stop words \n",
    "\n",
    "#Getting Stopwords from NLTK   \n",
    "nltk.download()  \n",
    "Must have to have a corpus from which it gets the stop words  \n",
    "\n",
    "from nltk.corpus import stopwords  \n",
    "sw = stopwords.words('english')\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **Step 3: Bag of Words**\n",
    "- Very known model\n",
    "- Used to preprocess the texts to classify before fitting the classification algorithms  \n",
    "- Take any text and count the frequency of words \n",
    "\n",
    "### **Properties of Bag of Words**\n",
    "- Just a frequency count  \n",
    "- Word order does not matter  \n",
    "- Long phrases give different input vectors  \n",
    "- Cannot handle complex phrases  \n",
    "- Advanced techniques can look at the stem of a word  \n",
    "\n",
    "### **Bag of Words in sklearn**  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "vectorizer = CountVectorizer()    \n",
    "\n",
    "string1= 'Hey Katie, the self driving car will be late Best, Sebastian'  \n",
    "string2 = 'Hey Sebastian, the machine learning class will be great great Best, Katie'\n",
    "string3 = 'Hey Katie, the machine learning class will be most excellent'\n",
    "\n",
    "email_list = [string1, string2, string3]  \n",
    "\n",
    "#Fit data using the vectorizer\n",
    "#figuring out what all the words in the corpus are and assigning list indices to each of them\n",
    "bag_of_words = vectorizer.fit(email_list)\n",
    "\n",
    "#Transform\n",
    "#takes all the words in the corpus and figures out the count of each word    \n",
    "bag_of_words = vectorizer.transform(email_list)    \n",
    "\n",
    "#Figure out feature number for each word \n",
    "print vectorizer.vocabulary_.get('great')    \n",
    "\n",
    "## **Tfidf Representation**   \n",
    "Tfidf rates the rare words more highly than the common words.   \n",
    "Because the rare words in general are going to be the one that helps you distinguishes the messages from each other.  \n",
    "\n",
    "Tf = Term Frequency \n",
    "- each word is upweight by how often it occurs in a document\n",
    "- like bag of words\n",
    "\n",
    "Idf = Inverse Document Frequency\n",
    "- the word also get a weighting that relates to how often it occurs in the corpus as a whole (in all the documents put together)\n",
    " \n",
    "vectorizer = TfidfVectorizer(sublienar_tf = True, max_df = .50, stop_words = 'english')   # ignore words that are in >50% of the documents; so common, no info      \n",
    "features_train_transformed = vectorizer.fit_transform(features_train)        \n",
    "features_test_transformed = vectorizer.transform(features_test)   \n",
    "\n",
    "\n",
    "--- \n",
    " \n",
    "\n",
    "## **Step 4: Fit Classification Algorithm**  \n",
    "- Most NLP algorithms are classification models : Logistic Regression, Na√Øve Bayes, CART (which is a model based on decision trees), Maximum Entropy again related to Decision Trees, Hidden Markov Models which are models bae on Markov processes.  \n",
    "\n",
    "\n",
    "### ** Model Selection Approaches**   \n",
    "\n",
    "**Approach 1: Could test each classification model and look at model performance criteria to decide best model **   \n",
    "\n",
    "Classification  Model Metrics  \n",
    "- Accuracy = TP + TN / TP + TN + FP + FN  \n",
    "- Precision (exactness) = TP / (TP + FP)  \n",
    "- Recall (completeness) = TP/ (TP + FN)  \n",
    "- F1 Score (completeness & recall compromise)= 2*Precision * Recall / (Precision + Recall)  \n",
    "        \n",
    "\n",
    "**Approach 2: Go with commonly used models for NLP: **\n",
    "- Naive Bayes, \n",
    "- Decision Tree or \n",
    "- Random Forest Classification  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
