{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION - PREDICT CONTINUOUS NUMBER\n",
    "Dependent variable y (outcome we're trying to predict) is continuous  \n",
    "\n",
    "Examples:   \n",
    "Try to learn to predict target (Y) given input (X)\n",
    "-  Trying to predict mm of rainfall tomorrow  \n",
    "-  Trying to predict the value of Google’s stock price  \n",
    "\n",
    "## **Properties**  \n",
    "- Output type: Continuous Number        \n",
    "- What are you trying to find: 'Best Fit Line'    \n",
    "- Evaluation: Sum of Squared Error or R-squared\n",
    "\n",
    "## Linear v Non-Linear Problem\n",
    "Linear: \n",
    "- Simple or \n",
    "- Multiple Linear Regression  \n",
    "\n",
    "Non-linear:   \n",
    "- Polynomial Regression, SVR, Decision Tree or Random Forest  \n",
    "- k-Fold Cross Validation and then pick model with the best results  \n",
    "\n",
    "## Outliers  \n",
    "### **What causes outliers?  **\n",
    "- Sensor Malfunction (Ignore)\n",
    "- Data Entry Errors (Ignore)\n",
    "- Freak Events (Pay Attention)  \n",
    "\n",
    "### **Outlier Rejection Loop  **    \n",
    "Fit the regression on all training points, discard the 10% of points that have the largest residual errors, and refit on the remaining points. \n",
    "\n",
    "**1. Train **\n",
    "\n",
    "**2. Remove points with the highest residual error after training**\n",
    "- residual error = error a data point has after you fit the best possible line  \n",
    "- approx. 10% of data points, but may vary from application to application  \n",
    "\n",
    "**3. Re-Train remaining points**\n",
    " \n",
    "Approach works for any machine learning alogrithm.  \n",
    "\n",
    "## Coding  \n",
    "### **Split > Fit > Predict > Score It**  \n",
    "Fit aka 'train'  \n",
    "Algorithms that can learn from observational data and make predictions based on it  \n",
    "\n",
    "### **Using 2 Main Functions **   \n",
    "train(x,y)  \n",
    "predict(x)\n",
    "\n",
    "### **Data Types and Shapes**    \n",
    "Sci-kit learn requires everything to be numerical  \n",
    "\n",
    "x is a **MATRIX** of n x D    \n",
    "X = dataset[['feature1','feature2','feature3']]\n",
    "- n = # of samples (observations)\n",
    "- d = number of features (dimensions)\n",
    "\n",
    "y is a **VECTOR** of shape n x 1  \n",
    "y = dataset['target']\n",
    "- for a regression y will be float values  \n",
    "\n",
    "### Generalization  \n",
    "- Generalization: predict accurately not only for data we trained on but new data we haven’t seen before  \n",
    "- Usually split data into train/test sets to get an idea of how well a mode will generalize  \n",
    "\n",
    "## **EVALUATION**    \n",
    "\n",
    "### ** Sum of the Squared Error**   \n",
    "Can compare SSE to figure out which line is being fit better\n",
    "\n",
    "Drawbacks:  \n",
    "- As you add more data, the SSE will almost certaintly go up but it doesn't necessary mean that your fit is doing a worse job.  \n",
    "- So if you're comparing two different sets of data that have a different number of points in them, this can be a big problem  \n",
    "as the SSE can be jerked around by the # of data points that you're using (even though the fit might be perfectly fine).  \n",
    "\n",
    "### **R-squared (Coefficient of Determination)**    \n",
    "Answers the question 'how much of my change in the output (y) is explained by the change in my input (x)'?\n",
    "- % of the total variation in dependent variable y is captured by the model  \n",
    "- Ranges from 0 to 1  \n",
    "- independent of the number of training points in the data set (so more reliable than SSE)  \n",
    "\n",
    "def compute_r_squared(data, predictions):    \n",
    "SST = ((data - np.mean(data))^2).sum()    \n",
    "SSReg = ((predictions - data))^2).sum()    \n",
    "r_squared = 1 - SSReg / SST    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " \n",
    "# **1. \tSIMPLE LINEAR REGRESSION - - LINEAR  **    \n",
    "Fit straight line to dataset of observations and use this line to predict unobserved values  \n",
    "One dimensional regression (only one input variable)  \n",
    "\n",
    "## **Sum of Squared Error (SSE)**  \n",
    "For supervised learning, we know there is a cost function to minimize.  \n",
    "The best regression is the one that minimizes the sum of the squared errors. \n",
    "SSE = Sum[(actual value - predicted value of the point on the regression line from regression)^2]   \n",
    "\n",
    "There can be multiple lines that minimize the sum of the absolute errors, but only one line will minimize sum of the squared errors!  So using SSE also makes implementation much easier.\n",
    "\n",
    "So in the equation y = mx + b,  want to find the m and b that will minimize SSE.    \n",
    "\n",
    "\n",
    "## Algorithms to Minimize SSE\n",
    "There are several algorithms that minimize the sum of the squared errors in a regression.   \n",
    "Estimate the coefficients of our linear model, usually using:\n",
    "1. Ordinary Least Squares\n",
    "2. Gradient Descent\n",
    "\n",
    "### **Ordinary Least Squares  **   \n",
    "Used in sklearn LinearRegression  \n",
    "Sometimes called ‘Maximum likelihood estimation’  \n",
    "The least squares method can effectively fit linear models since it only requires matrix algebra and provides deterministic estimates of the coefficients.     \n",
    " \n",
    "Minimizes the squared-error between each point and the line    \n",
    "- Error is just the distance between each point and the line  \n",
    "- Sum up all those squared errors  \n",
    "- Measuring the variance of the data points from that line  \n",
    "- By minimizing the variance, find the line that fits best\n",
    "\n",
    "**Pros: **  \n",
    "Works on any size of data set, gives information about relevance of features  \n",
    "Is always gauranteed to find the optimal solution when performing linear   regression\n",
    "\n",
    "**Cons: **  \n",
    "Linear regression assumptions  \n",
    "Least squares is a method which directly minimizes the sum of square error in a model algebraically. Often times we have too much data to fit into memory and we can't use least squares. \n",
    "\n",
    "### **Gradient Descent**  \n",
    "Alternative method to least squares  \n",
    "Gradient descent is a general method that can be used to estimate coefficents of nearly any model, including linear models.   \n",
    "At it's core, gradient descent minimizes the residuals in the estimated model by updating each coefficent based on it's gradient. \n",
    "Looks for the global minimum\n",
    "\n",
    "\n",
    "**Pros:**\n",
    "- Works best with 3D data  \n",
    "- Iterates to find line that best follows the contours defined by the data  \n",
    "- Easy to try in Python and compare to Least Squares \n",
    "\n",
    "**Cons**  \n",
    "- Not always gauranteed to find the optimal value.  Usually Least Squares is a perfectly good choice  \n",
    "\n",
    "\n",
    "- Cost function may have numerous local minima. When using gradient descent, our algorithm can become trapped in a local minimum that is not actually the global minimum of the cost function.  May need to perform gradient descent numerous times, randomizing our initial values of theta with idfferent random values every time (remember to seed random values for repeatability).   \n",
    " \n",
    "### Coding\n",
    "\n",
    "#Split  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .20, random_state= 0)  \n",
    "\n",
    "#Fit  \n",
    "from sklearn.linear_model import LinearRegression  \n",
    "reg = LinearRegression()  \n",
    "reg.fit(ages, net_worths)  \n",
    "\n",
    "#Predict  \n",
    "print 'katie's net worth prediction:', reg.predict([27])  \n",
    "\n",
    "#Score It    \n",
    "print 'r-squared score:', reg.score(ages, net_worths)  \n",
    "print 'slope:', reg.coef_    \n",
    "print 'intercept:', reg.intercept_   \n",
    "\n",
    "#Visualization  \n",
    "plt.scatter(ages, net_worths)  \n",
    "plt.plot(ages, reg.predict(ages), color = 'blue', linewidth = 3)  \n",
    "plt.xlabel('ages')  \n",
    "plt.ylabel('net worths')\n",
    "plt.show()\n",
    "  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **2.\tMULTIPLE LINEAR REGRESSION - -LINEAR **  \n",
    "Also called Multi-Variate Regression  \n",
    "More than one input variable\n",
    "Create coefficients in front of numerical values that represent each feature (still using least squares)     \n",
    "\n",
    "\n",
    "### Pros:   \n",
    "- Works on any size of data set, gives information about relevance of features    \n",
    "- Can still measure fit with r-squared   \n",
    "\n",
    "### Cons: \n",
    "- Linear regression assumptions\n",
    "- Normalize features so you can compare coefficients in a meaningful way  \n",
    "- Can’t really use categorical data (ordinal data you can) \n",
    "\n",
    "---\n",
    "\n",
    "## **3.\tPolynomial Regression - - NON LINEAR**  \n",
    "Linear formula y = mx + b is a first degree polynomial   \n",
    "\n",
    "### Pros:  \n",
    "- Works on any size of dataset, works very well on non-linear problems    \n",
    "- Higher orders produce more complex curves    \n",
    "- Can still measure fit with r-squared    \n",
    "\n",
    "### Cons:   \n",
    "- Need to choose the right polynomial degree for a good bias/variance tradeoff  \n",
    "\n",
    "---\n",
    "\n",
    "## **4.\tSupport Vector Regression- - NON LINEAR**  \n",
    "a.\tPros: Easily adaptable, works very well on non-linear problems, not biased by outliers  \n",
    "b.\tCons: Compulsory to apply feature scaling, not well known, more difficult to understand  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **5.\tDecision Tree Regression- - NON LINEAR**  \n",
    "a.\tPros: Interpretability, no need for feature scaling, works on linear / nonlinear problems  \n",
    "b.\tCons: Poor results on too small datasets, overfitting can easily occur  \n",
    "\n",
    "---\n",
    "\n",
    "## **6.\tRandom Forest Regression- - NON LINEAR**  \n",
    "a.\tPros: Powerful and accurate, good performance on many problems, incl. non linear  \n",
    "b.\tCons: No interpretability, overfitting can easily occur, need to choose number of trees  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
