{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEEP LEARNING INTUITION**\n",
    "\n",
    "Mimic the human brain – neurons    \n",
    "\n",
    "\n",
    "**INPUT VALUES **   \n",
    "- Incorporate value right away to proceed to the next round independent variables of one observation (1 row in database)    \n",
    "- need to standardize them (mean of zero and variable of one)    \n",
    "- or sometimes normalize (subtract min value divide by range to get values between 0-1)    \n",
    "- basically want all of these values to be similar to make it easier for neural network to process them  \n",
    "\n",
    "\n",
    "**HIDDEN LAYERS **\n",
    "\n",
    "\n",
    "** OUTPUT VALUES**      \n",
    "- Continuous (price)    \n",
    "- Binary (will exit yes/no)    \n",
    "- Categorical (several output values)    \n",
    "\n",
    "**WEIGHTS**     \n",
    "- How neural networks learn    \n",
    "- Things that get adjusted through the process of learning   \n",
    "\n",
    "**FUNCTIONS**\n",
    "\n",
    "**The Activation Function**\n",
    "\n",
    "\n",
    "**Threshold Function**    \n",
    "- Goes from zero to one    \n",
    "- If value is less than or more than 0    \n",
    "- Kinks in its curve    \n",
    "\n",
    "\n",
    "**Sigmoid Function**    \n",
    "- Goes from zero to one    \n",
    "- Function used in logistic regression    \n",
    "- Smooth, gradual progression    \n",
    "- Useful when trying to predict probability    \n",
    "\n",
    "**Rectifier**  \n",
    "- One of the most used functions  \n",
    "\n",
    "**Hyperbolic Tangent function **   \n",
    "- Similar to Sigmoid Function    \n",
    "- Goes below zero    \n",
    "---\n",
    "**NEURAL NETWORKS**  \n",
    "Many machine learning prediction problems are rooted in complex data and its non-linear relationships between features. Neural networks are a class of models that can learn these non-linear interactions between variables.\n",
    "\n",
    "Neural networks are very loosely inspired by the structure of neurons in the human brain. These models are built by using a series of activation units, known as neurons, to make predictions of some outcome. Neurons take in some input, apply a transformation function, and return an output. Below we see a representation of a neuron.\n",
    "\n",
    "To benchmark how well a three layer neural network performs when predicting the species of iris flowers, you will have to compute the AUC, area under the curve, score of the receiver operating characteristic. The function NNet3 not only trains the model but also returns the predictions. The method predict() will return a 2D matrix of probabilities. Since there is only one target variable in this neural network, select the first row of this matrix, which corresponds to the type of flower.\n",
    "\n",
    "---\n",
    "\n",
    "**HOW DO NEURAL NETWORKS LEARN**      \n",
    "Sum of squared differences between y-hat and y is back propagated through the neural network    \n",
    "Weights are adjusted accordingly    \n",
    "\n",
    "\n",
    "**Brute Force:**\n",
    "try all the different combinations  \n",
    "\n",
    "\n",
    "**Gradient Descent**   \n",
    "Gradient descent is a general method that can be used to estimate coefficents of nearly any model, including linear models. At it's core, gradient descent minimizes the residuals in the estimated model by updating each coefficent based on it's gradient.  \n",
    "- Convex (bowl shaped)    \n",
    "- Get to the bottom by understanding which way to go    \n",
    "- Which way is does it feel its going downward and take steps towards that    \n",
    "- Descending to the minimum of the cost function    \n",
    "\n",
    "**Batch Gradient Decent**    \n",
    "- Adjusting weights after you run all the rows in your neural network, iterate and re-run    \n",
    "- Deterministic algorithm    \n",
    "- As long as you have the same starting weights, every time you run it get the same iterations and results of the way your weights are being updated    \n",
    "\n",
    "\n",
    "**Stochastic Gradient Descent **   \n",
    "- Stochastic = randomly determined    \n",
    "- Doesn’t require cost function to be convex (bowl shaped)    \n",
    "- Run one row at a time and adjust the weights then repeat    \n",
    "- Doing one iteration one row at a time    \n",
    "- Helps avoid problem of finding local minimums rather than the overall global minimum    \n",
    "- Faster and lighter algorithm    \n",
    "\n",
    "**Stochastic Algorithm**    \n",
    "- Picking rows in random manner    \n",
    "- Even if you have same weights at the start, have different process/iterations    \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**TRAINING THE ANN WITH STOCHASTIC GRADIENT DESCENT**   \n",
    "1.\tRandomly initialize the weights to small numbers close to 0 (but not 0)    \n",
    "2.\tInput the first observation of your dataset in the input layer, each feature in one input node    \n",
    "3.\tForward Propagation    \n",
    "a.\tFrom left to right, the neurons are activated in a way that the impact of each neuron’s activation is limited by the weights \n",
    "b.\tPropagate the activations until getting the predicted result y    \n",
    "4.\tCompare the predicted result to the actual result.  Measure the  generated error    \n",
    "5.\tBack-Propagation:     \n",
    "a.\tFrom right to left, the error is back-propagated    \n",
    "b.\tUpdate the weights according to how much they are responsible for the error    \n",
    "c.\tThe learning rate decides by how much we update the weights    \n",
    "6.\tRepeat Steps 1-5     \n",
    "a.\tupdate the weights after each observation (Reinforcement Learning)    \n",
    "b.\tupdate the weights only after a batch of observations (Batch Learning)    \n",
    "7.\tWhen the whole training set passed through the ANN, that makes an epoch. Redo more epochs      \n",
    "\n",
    "\n",
    "---\n",
    "**ANN IN PYTHON**    \n",
    "- Encode categorical variables     \n",
    "- Only if it’s 3 or more categories    \n",
    "- Don’t need to for two categories because one will automatically be left out to avoid dummy trap    \n",
    "\n",
    "**Number of nodes in hidden layer**    \n",
    "- If data is linearly separable, don’t need a hidden layer or a neural network at all    \n",
    "- Choose # of nodes in hidden layer as the # of nodes in input (# independent variables) & output layer/2  \n",
    "- Or Parameter tuning     \n",
    "\n",
    "**K – fold cross validation**     \n",
    "- creating set in dataset besides training set and test set “cross validation set”    \n",
    "- experiment with different paramaters in model in cross validation set    \n",
    "\n",
    "**Initialize weights**    \n",
    "- ‘uniform’:     \n",
    "\tinitialize weights randomly and    \n",
    "\t makes sure they are close to zero    \n",
    "- Activation function    \n",
    "- Hidden layers: rectifier function ‘relu’    \n",
    "- Output layer: sigmoid function    \n",
    "\n",
    "**Output layer**    \n",
    "- Want to have probabilities of the outcome    \n",
    "- In order to get probabilities, have to replace activation function from rectifier to sigmoid function    \n",
    "- Dependent later with more than 2 categories    \n",
    "\tOutput_dim = 3    \n",
    "\tActivation function = softmax (sigmoid function but applied to dependent variable with more than 2 categories)    \n",
    "- Compile    \n",
    "\n",
    "---\n",
    "\n",
    "**CONVOLUTIONAL NEURAL NETWORKS **     \n",
    "Input Image > CNN > Output Label (Image Class) \n",
    "\n",
    "\n",
    "**1.Convolution**    \n",
    "a.\tGo through input image by column (stride) with Feature Detector box and look for matches\n",
    "b. Reduces input image but keep important features    \n",
    "c.\tInput Image > Feature detector > Feature map    \n",
    "d.\tFind features in your in your image using the feature detector and put them into a feature map    \n",
    "e.\tReLu Layer    \n",
    "    i.\tApply Rectifier function    \n",
    "    ii.\tWant to increase non-linearity in our CNN  \n",
    "    iii. Images themselves are highly non-linear      \n",
    "\n",
    "\n",
    "**2.Max Pooling (Down sampling)**    \n",
    "a.\tFeature Map > Max Pooling > Pooled Feature Map    \n",
    "b.\tTake the max of each 4x4 box, stride of 2     \n",
    "c.\tPreserve the features and account for possible spatial or textural distortions    \n",
    "i.\tIntroducing spatial invariance    \n",
    "d.\tAlso reducing the side    \n",
    "e.\tReducing number of parameters that are going  into the final layers of CNN    \n",
    "i.\tPreventing overfitting    \n",
    "f.\tLess info, but same features \n",
    "\n",
    "\n",
    "**3.Flattening**    \n",
    "a.\tFlatten into column    \n",
    "b.\tTake numbers row by row and put them into a single column    \n",
    "c.\tVector of inputs for CNN    \n",
    "d.\tInput image > Convolution into Convolutional Layer > Pooling into Pooling Layer > Flattening into input layer of a future   \n",
    "\n",
    "\n",
    "**ANN**    \n",
    "4.\tFull Connection      \n",
    "- Adding new ANN to CNN      \n",
    "- In CNN hidden layers are called Fully Connected Layer      \n",
    "---\n",
    "**Summary **     \n",
    "- Input image to which we apply multiple different feature detectors (filters) to create feature map.  This comprises our convolutional layer then on top of that convolutional layer we apply the ReLu to remove linearity In our images.    \n",
    "- Then apply pooling layer to convolutional layer to make sure we have spatial invariance in our images (can still pick up features even if its tilted) and reduce size and avoid over-fitting (max pooling)    \n",
    "- Flatten pooled images into one long vector (column of values) and input that into ANN    \n",
    "- Fully connected ANN all the features are processed through the network and we have this final fully connected layer which performs the voting towards the classes we’re after.  All of this is trained through forward and back propogation with lots of iterations and epochs and in the end we have a well defined NN.    \n",
    "- Not only are the weights trained, feature detectors are trained and adjusted in the same gradient descent process and that allows us to come up with the best feature maps. And in the end, we get a fully trained CNN which can recognize images and classify them.      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
