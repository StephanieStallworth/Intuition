{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING (aka Online Learning)\n",
    "- Used to solve interacting problems where the data observed up to time t is considered to decide which action to take at time t + 1.  \n",
    "- It is also used for Artificial Intelligence when training machines to perform tasks such as walking.   \n",
    "- Desired outcomes provide the AI with reward, undesired with punishment.   \n",
    "- Machines learn through trial and error.  \n",
    "\n",
    "\n",
    "**The Multi-Armed Bandit Problem**  \n",
    "- 5 or more slot machines but need to find out how to bet to max your return  \n",
    "- Because you don’t know which is the best  \n",
    "- Max your return as you’re exploring until you find the best one and exploit that one  \n",
    "- Combine exploration & exploitation & get to optimal results ASAP to max output of efforts  \n",
    "- Ideal trade-off between exploration and exploitation  \n",
    "- Exploit the best one while you’re exploring for it  \n",
    "- Find out which is the best one in the quickest way possible and start exploiting it along the way (as you’re exploring)  \n",
    "\n",
    "Upper Confidence Bound Algorithm solves Multi-Armed Bandit Problem  \n",
    "\tBetter than AB tests  \n",
    "\tDeterministic algorithm  \n",
    "\tRequires update at every round  \n",
    "  \n",
    "**Thompson Sampling Algorithm **   \n",
    "- Incorporate value right away to proceed to the next round \n",
    "\n",
    "**Probabilistic Algorithm**  \n",
    "- Can accommodate delayed feedback - Update in a batched manner  \n",
    "\tBetter empirical evidence  \n",
    "- You have some sort of agent that “explores” some space  \n",
    "- As it goes, it learns the  value of different state changes in different conditions  \n",
    "- Those values inform subsequent behavior of the agent  \n",
    "- Examples: Pac-Man  \n",
    "- Yields fast on-line performance on the space has been explored.  \n",
    "\n",
    "\n",
    "**Q Learning**  \n",
    "- A specific implementation of reinforcement learning  \n",
    "- You have:  \n",
    "\tA set of environmental states  \n",
    "\tA set of possible actions in those states  \n",
    "\tA value of each state/action Q  \n",
    "- Start off with Q values of 0   \n",
    "- Explore the space  \n",
    "- As bad things happen after a given state/action, reduce its Q  \n",
    "- As rewards happen after a given state/action, increase its Q \n",
    "\n",
    "\n",
    "**Markov Decision Process**  \n",
    "- Provide a mathematical framework for modeling decision making in situations where outcomes are partly random/partly under control of a decision maker  \n",
    "- Describes process above using mathematical notation  \n",
    "- You can make an intelligent Pac-man in a few steps  \n",
    "- Have it semi-randomly explore different choices of movements (actions) given different conditions (states)  \n",
    "- Keep track of the reward or penalty associated with each choice for a given state/action (Q)  \n",
    "- Use those stored Q values to inform its future choices  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
