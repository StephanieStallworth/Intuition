{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATA PRE-PROCESSING INTUITION**\n",
    "\n",
    "Python Packages\n",
    "- pandas - Handle data in a way suited to analysis; Since you are an R user, you will love pandas as it supports dataframes.\n",
    "- matplotlib - Very good for plotting graphs and figures.\n",
    "- scikit-learn - For machine learning, look no further.\n",
    "- numpy: Multi-dimensional arrays and Matrices; Mathmatical functions. \n",
    "- and scipy - Amazing modules for scientific computing.\n",
    "- beautifulsoup4 - web scraping.\n",
    "- django - web framework for building web applications in Python. \n",
    "\n",
    "\n",
    "# **DATA WRANGLING**  \n",
    "\n",
    "Data Wrangling: Process of gathering, extracting, cleaning, and storing data.\n",
    "\n",
    "We need to assess our data to:\n",
    "\n",
    "- Test assumptions about:  \n",
    "    - Values that are there  \n",
    "    - Data types  \n",
    "    - Shape\n",
    "- Identify errors or outliers   \n",
    "- Find missing values  \n",
    "\n",
    "## DATA EXTRACTION \n",
    "Acquiring data often isn't fancy     \n",
    "Find stuff on the internet!      \n",
    "Alot of data stored in textfiles and on govt websites      \n",
    "\n",
    "\n",
    "### **CSV: Comman Separated Value**  \n",
    "\n",
    "Tabular data  \n",
    "Row: item in a dataset  \n",
    "Column: fields for the data items  \n",
    "Cells: Individual values for a field    \n",
    "\n",
    "**Why CSV formats are used**  \n",
    "Lightweight  \n",
    "Each line of text is a single row  \n",
    "Fields are separated by a delimeter  \n",
    "Just the data (and delimeters) itself    \n",
    "\n",
    "Like a spreadsheet with no formulas,    \n",
    "Easy to process with code (unlike .xlsx)     \n",
    "All spreadhseet apps read/write csv  \n",
    "Don't need special software (ie, Excel)  \n",
    "- If the file is big, opening  it in a spreadsheet app like Excel can be slow, inefficient, or maybe even impossible. \n",
    "- May also want to programatically process tabular data because we may have alot of files to process and doing it manually in a spreadsheet app isn't possible.   \n",
    "\n",
    "**Python Code**\n",
    "#To read a CSV  \n",
    "pd.read_csv('baseball_data')\n",
    "\n",
    "#To write to a CVS  \n",
    "baseball_data.to_csv('baseball_data_with_weight_height.csv')    \n",
    "\n",
    "## XLRD  \n",
    "Excel files  \n",
    "\n",
    "## **XML**   \n",
    "resembles html    \n",
    "\n",
    "**XML Design Goals: **   \n",
    "Data transfer that is platform independent      \n",
    "Easy to write code to read/write      \n",
    "Document validation    \n",
    "\n",
    "**XML Standard:**  \n",
    "Robust parsers in most languages    \n",
    "We can focus on our app  \n",
    "It's also free, as in beer   \n",
    "\n",
    "**Best Practices for Scraping**\n",
    "1. Look at how browser makes requests  \n",
    "2. Emulate in code  \n",
    "\n",
    "## **JSON**  \n",
    "Looks like a Python dictionary with curly braces (value is associated with a key).   \n",
    "Supports nested structures in a way that csv documents cannot.\n",
    "\n",
    "Data Modeling in Json  \n",
    "Items may have different fields  \n",
    "May have nested objects  \n",
    "May have nested arrays  \n",
    "\n",
    "Most like to encounter json data through a web service.  A web service is a database you can access using http requests\n",
    "\n",
    "**Python Code**    \n",
    "\n",
    "#Imports  \n",
    "import json  \n",
    "import requests  \n",
    "\n",
    "#Provide URL we should make an API call  \n",
    "url = 'http://www.mylink.com'  \n",
    "\n",
    "#Make our API call using the requests library and load the results into a dictionary  \n",
    "data = requests.get(url).text     \n",
    "data = json.loads(data)  \n",
    "print type(data)    \n",
    "\n",
    "#Print out the name of the no. 1 top artist  \n",
    "print data['topartists']['artist'][0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DATA CLEANING\n",
    "### Reality is: much of your time as a data scientist will be spent preparing and ‘cleaning’ your data of: outliers, missing data, malicious data, erroneous data, irrelevant data, inconsistent data and formatting  \n",
    "\n",
    "### Sources of Dirty Data\n",
    "User entry errors  \n",
    "Poorly applied coding standards    \n",
    "Different Schemas    \n",
    "Legacy systems    \n",
    "Evolving applications  \n",
    "No unique identifiers    \n",
    "Data migration  \n",
    "Programmer error  \n",
    "Corruption in transmission   \n",
    "\n",
    "### **Sanity Checking Data**  \n",
    "Does the data make sense?  \n",
    "Is there a problem?  \n",
    "Does the data look like I expect it to?  \n",
    "\n",
    "#Look at mean, min, and max values, an any differences in count  \n",
    "dataset.describe() \n",
    "\n",
    "### Measures of Data Quality  \n",
    "1. Validity: Conforms to a schema  \n",
    "2. Accuracy: Conforms to gold standard    \n",
    "3. Completeness: All records?    \n",
    "4. Consistency: Matches other data  \n",
    "5. Uniformity: Same units  \n",
    "\n",
    "### Blueprint for Cleaning  \n",
    "- Audit your data    \n",
    "- Create a data cleaning plan\n",
    "   - Identify causes\n",
    "   - Define Operations\n",
    "   - Test  \n",
    "- Execute the plan \n",
    "- Manually correct  \n",
    "\n",
    "Iterate until you have confidence in your data \n",
    "\n",
    "### **Types of Corrections**  \n",
    "Removing/correcting typos  \n",
    "Validating against known entities    \n",
    "Data enhancement    \n",
    "Data harmonization (St v Street)   \n",
    "Changing reference data (USA) \n",
    "\n",
    "---\n",
    "\n",
    "## **MISSING VALUES**  \n",
    "\n",
    "### **Reasons for Missing Values**\n",
    "- Occasional failures: Occasional system errors prevent data from being recorded or Nonresponse        \n",
    "- Some subset of subjects or event types are systematically missing certain data attributes, or missing entirely.  \n",
    "\n",
    "Important point: if the missing values from your data are distributed at random, your data may still be representative of the population. However if values are missing systematically, it could invalidate your findings.  Check your data to see if such effects are present. \n",
    "\n",
    "### **Identify Missing Values**  \n",
    "\n",
    "#Visualize missing values  \n",
    "sns.heatmap(dataset.isnull(),yticklabels = False, cbar = False, cmap = 'viridis')  \n",
    "plt.show()  \n",
    "\n",
    "#Count of missing values  \n",
    "dataset.isnull().sum()    \n",
    "\n",
    "#Percentage of values missing  \n",
    "dataset.isnull().sum()/len(dataset)*100  \n",
    "\n",
    "### **Dealing with Missing Data**    \n",
    "\n",
    "### **1.Partial Deletion**  \n",
    "Limiting our data set for analysis to the data we have available to us  \n",
    "- **Listwise Deletion:** exclude a particular datapoint from **ALL** analysis even if some useful values are present  \n",
    "- **Pairwise Deletion:** exclude a particular case from the analysis for tasks which are not possible with the data at hand \n",
    "\n",
    "### **2.Impution**  \n",
    "Used in scenarios which we do not have much data or removing missing values will compromise the representativeness of our sample.    \n",
    "Imputing and Linear Regression are two methods that are simple and relatively effective but can have negative side effects (obscure or amplify trends).     \n",
    "\n",
    "**Imputing the mean **  \n",
    "Drawback: Lessens correlations between variables     \n",
    "\n",
    "#Impute based on column mean  \n",
    "baseball['weight'] = baseball['weight'].fillna(numpy.mean(baseball['weight']))      \n",
    "      \n",
    "#Imputate mean based on condition     \n",
    "def impute_age(cols):    \n",
    "    Age = cols[0]    \n",
    "    Pclass = cols[1]    \n",
    "    if pd.isnull(Age):    \n",
    "        if Pclass == 1:    \n",
    "            return 37    \n",
    "        elif Pclass == 2:    \n",
    "            return 29    \n",
    "        else:    \n",
    "            return 24    \n",
    "    else:    \n",
    "        return Age    \n",
    "        \n",
    "#Apply the function to the Age column    \n",
    "train['Age']=train[['Age','Pclass']].apply(impute_age, axis =1 )    \n",
    "\n",
    "**Imputing Using Linear Regression **  \n",
    "Fit linear model to estimate the missing values  \n",
    "Drawbacks:  Overemphasize existing trends in the data (imputed values will amplify this trend); Exact values suggest too much certainty  \n",
    "\n",
    "### 3. Remove From Dataset Completely  \n",
    "\n",
    "#Remove any rows missing from a specific column  \n",
    "dataset = dataset[pd.notnull(dataset['Price'])]  \n",
    "\n",
    "#Remove all rows missing a value in any column  \n",
    "dataset = dataset.dropna() # set axis to 1 to drop entire columns that have a missing value  \n",
    "\n",
    "#Remove specfic column with alot of missing values      \n",
    "train.drop('Cabin', axis = 1, inplace = True)  \n",
    "\n",
    "---\n",
    "\n",
    "## **DEALING WITH OUTLIERS**   \n",
    "\n",
    "### **Identify Outliers**    \n",
    "\n",
    "**Kurtosis (Height of Peaks)**  \n",
    "Can be used to check for outliers     \n",
    "High kurtosis may indicate problems with outliers  \n",
    "\n",
    "**Standard Deviation**  \n",
    "Identify outliers by looking at the number of standard deviations from the median (or mean) in a more principled manner rather   than an arbitrary cutoff.    \n",
    "What multiple? You just have to use common sense    \n",
    "  \n",
    "**Outliers Function Using Standard Deviation**      \n",
    "def reject_outliers(data):    \n",
    "    u = np.median(data)    \n",
    "    s = np.std(data)    \n",
    "    filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]    \n",
    "    return filtered    \n",
    "filtered = reject_outliers(incomes)    \n",
    "plt.hist(filtered, 50)    \n",
    "plt.show()      \n",
    "\n",
    "---\n",
    "## **CATEGORICAL DATA **   \n",
    "Machine Learning algorithms require everything to be numeric.  \n",
    "With qualitative (categorical) variables we can implement two methods:\n",
    "1. Enumerate  \n",
    "2. Create dummy variables  \n",
    "\n",
    "### Enumerate  \n",
    "First one is to check distribution of the variable with respect to variable values and enumerate them.    \n",
    "\n",
    "**Replacing Column values in a dataframe**    \n",
    "adh['Gender'] = adh['Gender'].map({'M': 0, 'F': 1})  \n",
    "\n",
    "### Dummy Variables\n",
    "Second to create dummy variable for each possible category.    \n",
    "\n",
    "#Convert categorical variables into \"dummy\" or indicator variables     \n",
    "dSex = pd.get_dummies(train['Sex'], drop_first = True) # drop_first prevents multi-collinearity    \n",
    "dEmbark = pd.get_dummies(train['Embarked'], drop_first = True)    \n",
    "\n",
    "#Add new dummy columns to data frame  \n",
    "train = pd.concat([train,dSex,dEmbark],axis = 1)  \n",
    "  \n",
    "#Drop unnecessary columns  \n",
    "train.drop(['Sex', 'Embarked','Name','Ticket'], axis = 1, inplace = True)    \n",
    "\n",
    "---\n",
    "\n",
    "## FEATURE SCALING   \n",
    "Feature Scaling = changing the range of features  \n",
    "\n",
    "\n",
    "### **Why Feature Scaling?**  \n",
    "Many algorithms compute the Eucilidean Distance between two observations and if one of the features is vastly larger than another, the distance will be biased towards that particular feature.  So for many machine learning algorithms, it's important to scale- or normalize (or standardize)-  the data before using it.  \n",
    "\n",
    "Need to ask yourself, if your model is based on several numerical attributes – are they comparable?  \n",
    "\n",
    "### **Read the docs**  \n",
    "- Mot data mining and machine learning techniques work fine with raw, un-normalized data but double check the one you’re using before you start  \n",
    "    - Some models are ok with data that is not normalized (regression)  \n",
    "- But some models may not perform well when different attributes are on very different scales  \n",
    "    - It can result in some attributes counting more than others  \n",
    "    - Bias in the attributes can also be a problem    \n",
    "\n",
    "  \n",
    "### ** Algorithms That Require Feature Scaling**    \n",
    "- SVM with RBF kernel\n",
    "- k-Means Clustering  \n",
    "\n",
    "### **Feature Scaling  Not Necessary For: **   \n",
    "- Decision Trees    \n",
    "- Linear Regression  \n",
    "\n",
    "### **Two Specific Methods Feature Scaling Methods:**\n",
    "1. Normalization  \n",
    "2. Standardization  \n",
    "\n",
    "## Normalization    \n",
    "Making the range of feature values between 0 and 1  \n",
    "Re-scaling features so that they always span comparable ranges  \n",
    "Still contain the same info but just expressed in different units\n",
    "Easiest technique of scaling features but is not so useful for data with outliers    \n",
    "\n",
    "**Normalization Formula**    \n",
    "x prime (new rescaled feature) = x - x min / x max - x min\n",
    "\n",
    "## **Standardizing Numerical Data**   \n",
    "Variable has been rescaled to have a mean of 0 and standard deviation of 1   \n",
    "Usually preferered because it is less outlier sensitive  \n",
    "\n",
    "**Standardization Formula**    \n",
    "For each value, subtract the mean and then divide by the standard deviation  \n",
    "\n",
    "**Standardization in sklearn**  \n",
    "Scikit-learn PCA implementation has a “whiten” option that does this for you. Use it!    \n",
    "Scikit-learn has a preprocessing module with handy normalize and scale functions    \n",
    "- Your data may have ‘yes’ and ‘no’ that needs to be converted to 1 and 0    \n",
    "\n",
    "### Don’t forget to re-scale your results when you’re done (to interpret the results you get)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXPLORATION / DATA VISUALIZATION**  \n",
    "Build intuition & Find patterns  \n",
    "\n",
    "\n",
    "\n",
    "### Plots for categorical data\n",
    "In [6]:  \n",
    "for c in qualitative:  \n",
    "    train[c] = train[c].astype('category')  \n",
    "    if train[c].isnull().any():  \n",
    "        train[c] = train[c].cat.add_categories(['MISSING'])  \n",
    "        train[c] = train[c].fillna('MISSING')    \n",
    "def boxplot(x, y, **kwargs):  \n",
    "    sns.boxplot(x=x, y=y)  \n",
    "    x=plt.xticks(rotation=90)  \n",
    "f = pd.melt(train, id_vars=['SalePrice'], value_vars=qualitative)  \n",
    "g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)  \n",
    "g = g.map(boxplot, \"value\", \"SalePrice\")    \n",
    "\n",
    "\n",
    "### Time Series Data  \n",
    "Loess Curve    \n",
    "Plotted over data points  \n",
    "Emphasize long term trends rather than year-to-year variability  \n",
    "Form of weighted regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
