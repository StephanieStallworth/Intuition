{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIG DATA INTUITION  \n",
    "\n",
    "\n",
    "Big data: data too large to fit on one disk (5T or more)  \n",
    "\n",
    "## **DATA WAREHOUSING OVERVIEW: ETL AND ELT**\n",
    "\n",
    "### **WHAT IS DATA WAREHOUSING**  \n",
    "•\tA large, centralized database that contains information from many sources  \n",
    "•\tOften used for business analysis in large corporations or organization  \n",
    "•\tQueried via SQL or tools (i.e. Tableau)  \n",
    "•\tOften entire departments are dedicated to maintaining a data warehouse  \n",
    "o\tData normalization is tricky  \n",
    "\tHow does all of this data relate to each other?  \n",
    "\tWhat views do people need?  \n",
    "o\tMaintaining the data feeds is a lot of work  \n",
    "o\tScaling is tricky  \n",
    "\n",
    "\t\n",
    "### **ETL vs ELT**  \n",
    "ETL and ELT refer to how data gets into a data warehouse  \n",
    "If you move from a monolithic database built on Oracle or MySQL to more modern distributed databases built on top of Hadoop you can take the transform stage and do that after you load in the raw data (as opposed to before) and that can end up being more simple and more scalable, more efficiently.  \n",
    "\n",
    "**ETL: EXTRACT,TRANSFORM LOAD**  \n",
    "•\tOld school way   \n",
    "o\tRaw data from operational systems is periodically extracted  \n",
    "o\tThen the data is transformed in to a schema needed by the DW  \n",
    "o\tFinally, the data is loaded into the data warehouse, ,already in the structure needed \n",
    "\n",
    "**ELT: EXTRACT, LOAD, TRANSFORM **  \n",
    "o\tToday a huge Oracle instance isn’t the only choice for a large data warehouse  \n",
    "o\tThings like Hive let you host massive databases on a Hadoop cluster  \n",
    "o\tOr you might store it in a large, distributed NoSQL data store  \n",
    "\tAnd query it using things like Spark or MapReduce  \n",
    "o\tThe scalability of Hadoop lets you flip the loading process on its head  \n",
    " \tExtract raw data as before  \n",
    "\tLoad it in as-is  \n",
    "\tThen use the power of Hadoop to transform it in place  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP REDUCE\n",
    "\n",
    "### What is MapReduce\n",
    "MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parellel distributed algorithm on a cluster. \n",
    "\n",
    "MapReduce is a method to process vast sums of data in parallel without requiring the developer to write any other code other than the mapper and reduce functions.\n",
    "\n",
    "Parallel programming model for processing large datasets across a cluster of computers.  \n",
    "\n",
    "\n",
    "### When to Use\n",
    "MapReduce only works for tasks where you hope to employ many workers simultaneously where who do not have knowledge of each other's actions.  \n",
    "\n",
    "MapReduce splits a large job up into several smaller chucks that each fit onto one machine and occur simultaneously.  These machines do not communicate with each other while performing their computations.  \n",
    "\n",
    "### How It Works \n",
    "\n",
    "Computations are done via two functions: Mappers & Reducers   \n",
    "\n",
    "Start out with a collection of documents or records  \n",
    "Send these documents in a distributed way to many mappers, which each perform the same mapping on their respective documents and produce a series of intermediate key value pairs.  Then shuffle these intermediate results and send all key value pairs of the same key to the same reducer for processing. We do this so that each reducer can produce one final key value pair for each key. \n",
    "\n",
    "## HADOOP  \n",
    "Hadoop is a common open source implementation of the MapReduce programming model Couples the mapReduce programming model with the distributed file system.  \n",
    "\n",
    "\n",
    "## HADOOP- BASED PRODUCTS\n",
    "To more easily allow programmer to complete complicated tasks using the processing power of Hadoop, there are many infrastructures out there that are either built on top of Hadoop or allow data access via Hadoop. \n",
    "\n",
    "### Hive  \n",
    "Developed by Facebook  \n",
    "Can run MapReduce jobs through a SQL-like querying language called the Hive querying language\n",
    "\n",
    "### Pig\n",
    "Originally developed at Yahoo  \n",
    "Excels in some areas Hive does not  \n",
    "Pig jobs are written in a procedural language called Pig Latin  \n",
    "- Ability to be more explicit about the execution of your data processing (which is not possible in a delcarative language like SQL syntax)  \n",
    "- Ability to split data pipline  \n",
    "\n",
    "### Others  \n",
    "Mahout - machine learning  \n",
    "Giraph - for graph analysis  \n",
    "Cassandra - hybrid of key value and column oriented database    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
