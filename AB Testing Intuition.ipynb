{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B TESTS INTUITION\n",
    "\n",
    "** WHAT ARE A/B TESTS**\n",
    "- Controlled experiments on a website to measure the impact of a given change  \n",
    "- A controlled experiment, usually in the context of a website   \n",
    "- You test the performance of some change to your website (the variant) and measure conversion relative to your unchanged site (the control)  \n",
    "- Test all sorts of things  \n",
    "o\tDesign changes  \n",
    "o\tUI flow  \n",
    "o\tAlgorithmic changes  \n",
    "o\tPricing changes  \n",
    "o\tYou name it  \n",
    "\n",
    "---\n",
    "\n",
    "**A/B TEST ISSUES**  \n",
    "**How do you measure conversion **   \n",
    "Ideally choose what you are trying to influence (optimize)  \n",
    "- Order amounts  \n",
    "- Profit  \n",
    "- Ad clicks  \n",
    "- Order quantity  \n",
    "\n",
    "\n",
    "**Attributing actions downstream from your change can be hard ** \n",
    "- Especially if you’re running more than one experiment \n",
    "\n",
    "\n",
    "\n",
    "**Variance is your enemy ** \n",
    "- There’s so much random variance in order amounts to begin with that your result could be just based on chance  \n",
    "- Sometimes you need to also look at conversion metrics with less variance   \n",
    "\tOrder quantities vs order dollar amounts, for example  \n",
    "- Just looking at the differences in means is not enough, need to take into account variance as well.  \n",
    "\n",
    "---\n",
    "\n",
    "**T- TESTS AND P VALUE**    \n",
    "\n",
    "**DETERMINING SIGNIFICANCE**    \n",
    "- How do we know the result is likely to be ‘real’ as opposed to just random variation?  \n",
    "- Was this just a result of random variance inherit in the data itself or are we seeing an actual statistically significant   change in behavior between our control group and test group \n",
    "\n",
    "**T-STATISTIC ( aka T Test) **   \n",
    "- A measure of the difference between two sets expressed in units of standard error  \n",
    "- The size of the difference relative to the variance in the data  \n",
    "- A high t value means there’s probably a real difference between two sets  \n",
    "\tHave to decide threshold  \n",
    "\tSign tells you if its good or bad change  \n",
    "- Assumes a normal distribution of behavior  \n",
    "\tThis is a good assumption if you’re measuring revenue as conversion  \n",
    "\tSee also more refined version of t-statistic  \n",
    "- Fisher’s exact test (for click-through rates)  \n",
    "- E-test (for transactions per user)  \n",
    "- Chi-squared test (for product quantities purchased)  \n",
    "\n",
    "**P VALUE**   \n",
    "- Probabililty of A and B satisfying the ‘null hypothesis’ (there is no real difference between control and treament’s behavior) - Low P-Value implies significance  (Low probability that your change had no effect ) \n",
    "- It is the probability of an observation lying at an extreme t-value assuming the null hypothesis  \n",
    "\n",
    "**Using P Values**  \n",
    "- Choose some threshold for ‘significance’ before your experiment  \n",
    "- Liklihood there is no real effect, just a result of random variance  \n",
    "\n",
    "**When your experiment is over ** \n",
    "- Measure your P-Value  \n",
    "- If it’s less than your significance threshold then you can reject the null hypothesis that the change has no effect  \n",
    "- Low p value means there is a low probability that this is just a result of random variance  \n",
    "o\t+ roll it out  \n",
    "o\t– discard it before you lose more money  \n",
    "- To declare significance need to see high t statistic (terms of standard deviations)  and p value low (lower than threshold below 5% or ideally below 1%)  \n",
    "\n",
    "---\n",
    "\n",
    "**HOW LONG DO I RUN AN EXPERIMENT?**  \n",
    "How do I know when I’m done with an A/B test\n",
    "\n",
    "\n",
    "**1.You have achieved significance ( + or -)** \n",
    "\n",
    "\n",
    "**2.You no longer observe meaningful trends in your p-value**  \n",
    "- That is, you don’t see any indication that your experiment will ‘converge’ on a result over time  \n",
    "- P value should come down over time, more data it gets the more significant your results should be getting  \n",
    "- Flat line or going all over the place tells you that p-value isn’t going anywhere and it doesn’t matter how long your run the experiment  \n",
    "- Need to agree up front in case you don’t see any trends in p-values, what’s the longest you’re willing to run this experiment  \n",
    "\n",
    "**3.You reach some pre-established upper bound on time**  \n",
    "\n",
    "---\n",
    "\n",
    "**A/B TEST GOTCHAS**  \n",
    "**Correlations does not imply causation**  \n",
    "- Even your low p-value on a well-designed experiment does not imply casuation  \n",
    "\tIt could still be random chance  \n",
    "\tOther factors could be at play  \n",
    "\tIt’s your duty to ensure business owners understand this\n",
    "\n",
    "\n",
    "**Novelty Effects**  \n",
    "- Changes to a website will catch the attention of previous users who are used to the way it used to be  \n",
    "\tThey might click on something simply because it is new  \n",
    "- Good idea to re-run experiments much later and validate     \n",
    "\tOften the old website will outperform the new one after a while  because it is a change  \n",
    "\tMeasure it again when its no longer novel  \n",
    "\n",
    "**Seasonal Effects**   \n",
    "- An experiment run over a short period of time may only be valid for that period of time  \n",
    "\tEx: consumer behavior near Christmas is very different than other times of year  \n",
    "\tAn experiment run near Christmas may not represent behavior during the rest of the year  \n",
    "\tLook at conversion metric behavior for seasonal fluctuations, want to avoid running experiment during those peaks and values \n",
    "\n",
    "\n",
    "**Selection Bias** \n",
    "- Sometimes your random selection of customers for A or B isn’t really random  \n",
    "\tFor example: assignment is based somehow on customer ID  \n",
    "\tBut customers with low ID’s are better customer than ones with high ID  \n",
    "- Run an A/A test periodically to check  \n",
    "- Audit your segment assignment algorithms  \n",
    "\n",
    "\n",
    "**Data Pollution**  \n",
    "- Are robots (both self-identified and malicious) affecting your experiment  \n",
    "\tGood reason to measure conversion based on something that requires spending real money  \n",
    "- More generally, are outliers skewing the result?  \n",
    "\n",
    "**Attribution Errors ** \n",
    "- Often there are errors in how conversion is attributed to an experiment  \n",
    "- Using a widely used A/B test platform can help mitigate that risk  \n",
    "\tIf yours is home-grown, it deserves auditing  \n",
    "\n",
    "\n",
    "**Watch for ‘gray areas’ ** \n",
    "- Are you counting purchases toward an experiment within some given time frame of exposure to it?  \n",
    "- It that time frame too large?  \n",
    "- Could other changes downstream form the change you’re measuring affect your results?  \n",
    "- Are you running multiple experiments at once?  \n",
    "\n",
    "**SUGGESTED READING**  \n",
    "Data science from scratch, Joel Grus  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
