{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING INTUITION\n",
    "\n",
    "## **MACHINE LEARNING IS...**\t\t\n",
    "The training of a model from data that generalizes a decision against a performance measure\t\t\n",
    "\n",
    "Preparing a decision making program like this is typically called \"training\"\t\t  \n",
    "\n",
    "training = teaching the model what the correct answer looks like  \n",
    "Where collected examples are called the training set and the program is referred to as a model\t\n",
    "\n",
    "**Example**  \n",
    "A computer program is said to learn from experience E\t\t\t\n",
    "with some respect to some class of tasks T\t\t\t\n",
    "and performance measure P\t\t\t\n",
    "if its performance at tasks in T\t\t\t\n",
    "as measure by P\t\t\t\n",
    "improves with experience E\t\n",
    "\n",
    "**HELPS YOU THINK CLEARLY ABOUT...**\t  \n",
    "\n",
    "**EXPERIENCE**\t  \n",
    "What data to collect   \n",
    "- Emails\t  \n",
    "- Tweets    \n",
    "\n",
    "**TASK**  \n",
    "What decisions the software needs to make     \n",
    "- Classification of spam or ham   \n",
    "- Classify a tweat that has not been published as goingto get retweets or not    \n",
    "\n",
    "**PERFORMANCE**\t\n",
    "How we will evalutate its results   \n",
    "- Accuracy as a percentage  \n",
    "- Classification accuracy, the number of tweets predicted correctly out of all tweets considered as a percentage\n",
    "\n",
    "---\n",
    "## ML PROCESS  \n",
    "End-to-end process of investigating data through a machine learning lens.  \n",
    "Learn how to extract/identify useful features that best represents your data, most commonly used ML algorithms today, and how to evaluate the performance of the ML algorithms.  \n",
    "\n",
    "Be able to:  \n",
    "- Deal with imperfect, real-world dataset  \n",
    "- Validate a ML result using test data  \n",
    "- Evaluate a ML result using quantitative metrics\n",
    "- Create, select, transform features and compare the performance of ML algorithms  \n",
    "- Tune ML algorithms for max performance  \n",
    "- Communicate your ML results clearly  \n",
    "\n",
    "**1. Data Set/ Question**\n",
    "- Do I have enough data?  \n",
    "- Can I define my question?\n",
    "- Enough/right features to answer question?  \n",
    "\n",
    "**2. Feature Selection: try to extract info from dataset in form of features  **\n",
    "- **Exploration:** inspect for correlations, outlier removal, imputation, cleaning\n",
    "- **Creation:** think about it like a human  \n",
    "- **Representation:** test vectorization, descretization  \n",
    "- **Transforms:** PCA, ICA finding new feature spaces that work better\n",
    "- **Selection:** KBest Percentile, Recursive Feature Elimination  \n",
    "- **Scaling:** Mean Subtraction, Min/max scaler, Standard scaler\n",
    "    \n",
    "**3. Algorithm & Parameter Tunes -  pick ML algorithm & feed those features into it**  \n",
    "- **Supervised, Ordered (or Continuous) Output:** Linear Regression, Lasso Regression, Decision Tree Regression, SV Regression  \n",
    "- **Supervised, Non-Ordered (or Discrete) Output:** Decision Tree, Naive Bayes, SVM, Ensembles, k-NN, LDA, Logistic Regression\n",
    "- **Unsupervised:** K-Means Clustering, Aspectral Clustering, PCA, mixture models / EM Algorithm, outlier detection  \n",
    "- **Tune Your Algorithm:** parameters of algorithm, visual inspection, performance on test data, GridSearchCV\n",
    "\n",
    "**4. Evaluation ** \n",
    "- ** Validate:** train/test split, k-fold, visualize  \n",
    "- **Pick Metrics:** SSE or r^2, precision, recall, F1 Score, ROC curve, custom bias/variance\n",
    "---\n",
    "## ** DETAILED ML APPROACH**\n",
    "The Systematic Process For Working Through Predictive Modeling Problems That Delivers Above Average Results\n",
    "\n",
    "Over time, working on applied machine learning problems you develop a pattern or process for quickly getting to good robust results. Once developed, you can use this process again and again on project after project. The more robust and developed your process, the faster you can get to reliable results. In this post, I want to share with you the skeleton of my process for working a machine learning problem. You can use this as a starting point or template on your next project.\n",
    "\n",
    "**5-Step Systematic Process**  \n",
    "1 Define the problem    \n",
    "2 Pepare data    \n",
    "3 Spot check algorithms  \n",
    "4 Improve results  \n",
    "5 Present results  \n",
    "\n",
    "There is a lot of flexibility in this process. For example, the “prepare data” step is typically broken down into analyze data (summarize and graph) and prepare data (prepare samples for experiments). The “Spot Checks” step may involve multiple formal experiments.  \n",
    "\n",
    "It’s a great big production line that I try to move through in a linear manner. The great thing in using automated tools is that you can go back a few steps (say from “Improve Results” back to “Prepare Data”) and insert a new transform of the dataset and re-run experiments in the intervening steps to see what interesting results come out and how they compare to the experiments you executed before.\n",
    "\n",
    "### **1. DEFINE THE PROBLEM**  \n",
    "I like to use a three step process to define the problem. I like to move quickly and I use this mini process to see the problem from a few different perspectives very quickly:\n",
    "\n",
    "**1.1 What is the problem?**  \n",
    "Describe the problem informally and formally and list assumptions and similar problems.\n",
    "\n",
    "**Informally:**   \n",
    "I need a program that will tell me which tweets will get retweets\n",
    "\n",
    "**Formally**  \n",
    "**Experience**  \n",
    "- What data to collect E\t\n",
    "- Tweets   \n",
    "\n",
    "\n",
    "** Task**\n",
    "- What decisions the software needs to make  \n",
    "- Classify a tweet that has not been published as going to get retweets or not  \n",
    "\n",
    "**Performance**\n",
    "- How we will evalutate its results   \n",
    "- Classification accuracy, the number of tweets predicted correctly out of all tweets considered as a percentage  \n",
    "\n",
    "\n",
    "**1.2 Why does the problem need to be solved?**  \n",
    "List your motivation for solving the problem, the benefits a solution provides and how the solution will be used.\n",
    "\n",
    "**1.3 How would I solve the problem?**  \n",
    "Describe how the problem would be solved manually to flush domain knowledge.\n",
    "\n",
    "\n",
    "### **2. PREPARE DATA**  \n",
    "I preface data preparation with a data analysis phase that involves summarizing the attributes and visualizing them using scatter plots and histograms.   \n",
    "\n",
    "I also like to describe in detail each attribute and relationships between attributes. This grunt work forces me to think about the data in the context of the problem before it is lost to the algorithms  \n",
    "\n",
    "The actual data preparation process is three step as follows:\n",
    "\n",
    "**2.1 Data Selection:**     \n",
    "Consider what data is available, what data is missing and what data can be removed.  \n",
    "\n",
    "\n",
    "**2.2 Data Preprocessing:**    \n",
    "Organize your selected data by formatting, cleaning and sampling from it.\n",
    "- Formatting\n",
    "- Cleaning\n",
    "- Sampling\n",
    "\n",
    "**2.3 Data Transformation:**  \n",
    "Transform preprocessed data ready for machine learning by engineering features using scaling, attribute decomposition and attribute aggregation.\n",
    "\n",
    "### **3. SPOT CHECK ALGORITHMS**   \n",
    "\n",
    "**How Do I Know Which Model to Choose? **  \n",
    "1.Look at your dependent variable to decide if it is a Regression,   Classification, or Clustering Problem    \n",
    "- Continuous Outcome: Regression Problem    \n",
    "- Categorical Outcome: Classification Problem    \n",
    "- No dependent variable: Clustering Problem     \n",
    "\n",
    "\n",
    "2.Is my problem a linear problem or non-linear Problem?    \n",
    "- GridSearchCV  \n",
    "\n",
    "**Spot Check Performance**  \n",
    "I use 10 fold cross validation in my test harnesses by default. All experiments (algorithm and dataset combinations) are repeated 10 times and the mean and standard deviation of the accuracy is collected and reported.  \n",
    "I also use statistical significance tests to flush out meaningful results from noise. Box-plots are very useful for summarizing the distribution of accuracy results for each algorithm and dataset pair.  \n",
    "\n",
    "I spot check algorithms, which means loading up a bunch of standard machine learning algorithms into my test harness and performing a formal experiment. I typically run 10-20 standard algorithms from all the major algorithm families across all the transformed and scaled versions of the dataset I have prepared.\n",
    "\n",
    "\n",
    "The goal of spot checking is to flush out the types of algorithms and dataset combinations that are good at picking out the structure of the problem so that they can be studied in more detail with focused experiments.  \n",
    "\n",
    "\n",
    "More focused experiments with well-performing families of algorithms may be performed in this step, but algorithm tuning is left for the next step.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **4. IMPROVE RESULTS**    \n",
    "Consists of choosing best parameters of machine learning models \n",
    "After spot checking, it’s time to squeeze out the best result from the rig. I do this by running an automated sensitivity analysis on the parameters of the top performing algorithms. I also design and run experiments using standard ensemble methods of the top performing algorithms. I put a lot of time into thinking about how to get more out of the dataset or of the family of algorithms that have been shown to perform well.   \n",
    "\n",
    "Again, statistical significance of results is critical here. It is so easy to focus on the methods and play with algorithm configurations. The results are only meaningful if they are significant and all configuration are already thought out and the experiments are executed in batch. I also like to maintain my own personal leaderboard of top results on a problem.\n",
    "\n",
    "  \n",
    "In summary, the process of improving results involves:  \n",
    "\n",
    "**Algorithm Tuning:**   \n",
    "where discovering the best models is treated like a search problem through model parameter space.  \n",
    "\n",
    "\n",
    "**Ensemble Methods: **  \n",
    "where the predictions made by multiple models are combined.\n",
    "\n",
    "**Extreme Feature Engineering:**  \n",
    "where the attribute decomposition and aggregation seen in data preparation is pushed to the limits.   \n",
    "\n",
    "### **5. PRESENT RESULTS**  \n",
    "The results of a complex machine learning problem are meaningless unless they are put to work. This typically means a presentation to stakeholders. Even if it is a competition or a problem I am working on for myself, I still go through the process of presenting the results.It’s a good practice and gives me clear learnings I can build upon next time.  \n",
    "\n",
    "The template I use to present results is below and may take the form of a text document, formal report or presentation slides.  \n",
    "\n",
    "**Context (Why): **    \n",
    "Define the environment in which the problem exists and set up the motivation for the research question.\n",
    "\n",
    "**Problem (Question):**   \n",
    "Concisely describe the problem as a question that you went out and answered.\n",
    "\n",
    "**Solution (Answer):**   \n",
    "Concisely describe the solution as an answer to the question you posed in the previous section. Be specific.\n",
    "\n",
    "**Findings:**  \n",
    "Bulleted lists of discoveries you made along the way that interests the audience. They may be discoveries in the data, methods that did or did not work or the model performance benefits you achieved along your journey.\n",
    "\n",
    "**Limitations: **  \n",
    "Consider where the model does not work or questions that the model does not answer. Do not shy away from these questions, defining where the model excels is more trusted if you can define where it does not excel.\n",
    "\n",
    "**Conclusions (Why+Question+Answer):**  \n",
    "Revisit the “why”, research question and the answer you discovered in a tight little package that is easy to remember and repeat for yourself and others.\n",
    "\n",
    "\t\t\t\n",
    "\t\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
